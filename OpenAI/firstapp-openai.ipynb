{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ['LANGCHAIN_API_KEY'] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = \"true\"\n",
    "os.environ['LANGCHAIN_PROJECT'] = os.getenv(\"LANGCHAIN_PROJECT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader #Used for web scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = WebBaseLoader(\"https://www.comet.com/site/blog/langchain-document-loaders-for-web-data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://www.comet.com/site/blog/langchain-document-loaders-for-web-data/', 'title': 'LangChain Document Loaders for Web\\xa0Data - Comet', 'description': 'The effectiveness of RAG hinges on the method used to retrieve documents. Explore 3 key LangChain document loaders + how they effect output', 'language': 'en-US'}, page_content='\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLangChain Document Loaders for Web\\xa0Data - Comet\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n \\n\\n\\n\\n\\n\\nskip to Main Content\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nEnterprise\\nProducts\\n\\nExperiment Management\\nArtifacts\\nModel Registry\\nModel Production Monitoring\\nLLMOps\\n\\n\\nDocs\\nPricing\\nCustomers\\nLearn\\n\\nResources\\nBlog\\nDeep Learning Weekly\\nLLM Course\\n\\n\\nCompany\\n\\nAbout Us\\nNews and Events\\n\\nEvents\\nPress Releases\\n\\n\\nCareers\\nContact Us\\nLeadership\\n\\n\\nLogin\\nGet Demo\\nTry Comet Free\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\nSearch\\n\\n\\nSubmit\\n\\n\\nHome ‚Ä∫ Blog ‚Ä∫ LangChain Document Loaders for Web\\xa0Data\\n\\n\\n\\n\\n\\n\\nLangChain Document Loaders for Web\\xa0Data\\n\\n\\n\\nWords By \\nHarpreet Sahota \\n\\n            November 30, 2023        \\n\\n\\n\\n\\n\\nAnd An Assessment of How They Impact Your ragas Metrics\\n\\nPhoto by Ilya Pavlov on\\xa0Unsplash\\nIf you‚Äôve ever wondered how the quality of information sourced by language models affects their outputs, you‚Äôre in the right place.\\xa0I‚Äôm trying to unpack how different document loaders in LangChain impact a Retrieval Augmented Generation (RAG) system.\\nWhy is this important?\\xa0\\nRAG is a game-changer. It cleverly combines retrieving information from external documents with the generative capabilities of language models. However, the effectiveness of this system hinges on one critical aspect\\u200a‚Äî\\u200athe method used to retrieve documents.\\nThis blog is about exploring and understanding this pivotal element.\\nWe‚Äôll focus on three key players in LangChain:\\n\\nWebBaseLoader\\nSeleniumURLLoader,\\nNewsURLLoader.\\n\\nEach has its approach to fetching information, and we will find out how these methods shape the final output of RAG models.\\nI invite you to join this exploration\\u200a‚Äî\\u200ait‚Äôs not just an exploration of code and algorithms but a journey to enhance the intelligence and responsiveness of AI systems.\\nüßëüèΩ\\u200düíª Let‚Äôs write some\\xa0code!\\nStart with some preliminaries and setting the environment.\\n%%capture\\r\\n!pip install langchain openai unstructured selenium newspaper3k textstat tiktoken faiss-cpu\\r\\n\\r\\nimport os\\r\\nimport getpass\\r\\nfrom langchain.document_loaders import WebBaseLoader, UnstructuredURLLoader, NewsURLLoader, SeleniumURLLoader\\r\\n\\r\\nimport tiktoken\\r\\nimport matplotlib.pyplot as plt\\r\\nimport pandas as pd\\r\\nimport nltk\\r\\nfrom nltk.tokenize import sent_tokenize, word_tokenize\\r\\nfrom nltk.corpus import stopwords\\r\\nfrom textstat import flesch_reading_ease\\r\\nfrom collections import Counter\\r\\n\\r\\nfrom langchain.embeddings.openai import OpenAIEmbeddings\\r\\nfrom langchain.vectorstores import FAISS\\r\\nfrom langchain.chat_models import ChatOpenAI\\r\\nfrom langchain.chains import RetrievalQA\\r\\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\\r\\nos.environ[\\'OPENAI_API_KEY\\'] = getpass.getpass(\"Input your Open AI Key:\")\\nFor this demonstration, we‚Äôll use this website.\\nwebsite = \"https://phys.org/news/2023-11-qa-dont-blame-chatbots.html\"\\nThe function below will load the website into a LangChain document object:\\ndef load_document(loader_class, website_url):\\r\\n    \"\"\"\\r\\n    Load a document using the specified loader class and website URL.\\r\\n\\r\\n    Args:\\r\\n    loader_class (class): The class of the loader to be used.\\r\\n    website_url (str): The URL of the website from which to load the document.\\r\\n\\r\\n    Returns:\\r\\n    str: The loaded document.\\r\\n    \"\"\"\\r\\n    loader = loader_class([website_url])\\r\\n    return loader.load()\\nUnderstanding the WebBaseLoader\\n\\nPhoto by Emile Perron on\\xa0Unsplash\\nWhen extracting text from websites, the WebBaseLoader in LangChain is a tool you need to know about.\\nIt‚Äôs like a skilled miner adept at digging through the layers of a website to retrieve the valuable textual content beneath. Let‚Äôs explain exactly how it works and what this means for embedding documents into a vector database.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nWant to learn how to build modern software with LLMs using the newest tools and techniques in the field? Check out this free LLMOps course from industry expert Elvis Saravia of\\xa0DAIR.AI!\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHow WebBaseLoader Retrieves Text\\nThe WebBaseLoader uses HTTP requests, a basic yet powerful way to communicate with web servers. Think of it as sending a letter to a website asking for its content. Once the website replies, WebBaseLoader takes over, sifting through the HTML\\u200a‚Äî\\u200athe foundational code of web pages.\\nThis is where BeautifulSoup, a Python library, comes into play. WebBaseLoader uses BeautifulSoup to parse the HTML, effectively reading and extracting the text. It‚Äôs like having a translator who can interpret the complex language of HTML and present you with just the readable text.\\nImpact on Document Embedding and Vector Databases\\nWhen this extracted text is embedded into a vector database, there are a few implications:\\n\\nQuality of Extracted Text: WebBaseLoader relies on HTML structure and excels with well-structured websites. However, it might struggle with JavaScript-generated dynamic content, which is increasingly common in modern web design. This means the text it retrieves is as good as the HTML it interprets.\\nEfficiency: WebBaseLoader is efficient and fast, handling multiple requests seamlessly. This efficiency translates into quicker embedding of documents into your vector database, which is crucial for large-scale applications.\\nRelevance: The relevance of the extracted text can vary. In cases where websites are loaded with ads or unrelated content alongside the main text, WebBaseLoader might fetch some noise and valuable data. This could impact the precision of your RAG system‚Äôs outputs.\\n\\nWhile it‚Äôs efficient and effective for static content, its performance can be limited by dynamic web elements. Remember the content you‚Äôre targeting as we dive into document embedding and vector databases.\\nIf you focus on static, well-structured websites, WebBaseLoader could be your workhorse in the RAG pipeline.\\nwb_loader_doc = load_document(WebBaseLoader, website)\\n\\xa0You can examine the extracted content from any of the loaders with the following pattern:\\nwb_loader_doc[0].page_content\\nGrasping the SeleniumURLLoader\\n\\nPhoto by Markus Spiske on\\xa0Unsplash\\nImagine a scenario where you need to extract text from a website as dynamic as a bustling city street\\u200a‚Äî\\u200achanging every moment, filled with interactive elements and content that loads as you scroll.\\nThe SeleniumURLLoader steps in, bringing a different skill set than the WebBaseLoader.\\nHow SeleniumURLLoader Retrieves Text\\nThe SeleniumURLLoader is like an undercover agent in the world of web browsers.\\nIt doesn‚Äôt just send a request to a website; it navigates the web as a user would. Using Selenium, a powerful tool for browser automation, it opens an actual browser window (in headless mode, meaning without a graphical interface) and interacts with the webpage. This ability to simulate user interactions is crucial for websites where content is rendered through JavaScript\\u200a‚Äî\\u200aa common scenario in modern web development.\\nBefore extracting the text, the loader waits for the page to load, including any dynamic content.\\nImpact on Document Embedding and Vector Databases\\n\\nComprehensive Text Retrieval: Since it interacts with web pages like a human user, SeleniumURLLoader can retrieve text that other methods might miss. This includes content that appears due to user interactions or is dynamically loaded by JavaScript.\\nPerformance Considerations: The thoroughness of SeleniumURLLoader comes at a cost. It‚Äôs slower and more resource-intensive than more straightforward HTTP request methods. When embedding documents into a vector database, this could mean longer processing times, especially for large volumes of data.\\nAccuracy and Relevance: The text retrieved by SeleniumURLLoader tends to be highly accurate and reflective of the user‚Äôs experience on the website. This can lead to more relevant and context-rich embeddings in your vector database, potentially enhancing the quality of your RAG system‚Äôs outputs.\\n\\nThe SeleniumURLLoader is your toolkit‚Äôs Swiss army knife for dealing with dynamic, JavaScript-heavy websites. It offers a depth of text retrieval unmatched by more straightforward methods but requires more resources and time.\\nA RAG pipeline is the ideal choice when your focus is on comprehensively capturing the essence of modern, interactive web pages.\\nselenium_loader_doc = load_document(SeleniumURLLoader, website)\\nWith the WebBaseLoader and SeleniumURLLoader covered, we‚Äôll next explore the NewsURLLoader, a specialized tool for news content.\\nUnveiling the NewsURLLoader in LangChain\\nNewsURLLoader is designed specifically for news articles.\\nHow NewsURLLoader Retrieves Text\\nThe NewsURLLoader doesn‚Äôt just fetch text; it‚Äôs adept at navigating through the unique structure of news articles.\\nUsing the newspaper library, a Python package tailored for news extraction, performs a more refined retrieval. This loader not only fetches the article but also understands the typical layout of news websites, effectively separating the main content from the clutter of ads and sidebars. Moreover, the NewsURLLoader can perform light NLP (Natural Language Processing) tasks.\\nThis means it doesn‚Äôt just hand you the text; it can also provide summaries and extract keywords, offering a more concise and focused insight into the content.\\nImpact on Document Embedding and Vector Databases\\n\\nTargeted and Clean Extraction: The NewsURLLoader is designed explicitly for news content, which means it can efficiently extract clean and relevant text from news articles. This leads to high-quality document embeddings, especially valuable for news-related queries in an RAG system.\\nNLP Enhancements: The optional NLP features of the NewsURLLoader add an extra layer of value. By embedding summarized content and key terms, your vector database can become more efficient, focusing on the essence rather than the bulk of news articles.\\nScope Limitation: While it‚Äôs a powerhouse for news content, the NewsURLLoader‚Äôs specialization is also its limitation. It‚Äôs different than the tool for general-purpose web scraping or for handling dynamic, interactive content like the SeleniumURLLoader.\\n\\nThe NewsURLLoader shines in its domain, making it an excellent choice for RAG systems focused on current events, journalism, or news analysis. It offers clean, concise, and relevant text extraction, with the bonus of NLP processing.\\nAnalyzing the content from each\\xa0loader\\nIn this analysis, you‚Äôll dive deep into the text extracted by the three document loaders: WebBaseLoader, SeleniumURLLoader, and NewsURLLoader.\\nYou‚Äôll compare their outputs based on specific metrics: the total number of characters, the count of alphanumeric characters, the number of newline characters, and the total number of tokens as determined by GPT-4 encoding.\\nThe goal is to quantitatively assess the nature and quality of text each loader extracts. This technical analysis will provide clear insights into the efficiency and accuracy of these loaders, helping us understand their impact on a Retrieval Augmented Generation system.\\nYou‚Äôll present our findings through concise bar plots, comparing each loader‚Äôs performance straightforwardly.\\ndef count_alphanumeric(text):\\r\\n    \"\"\"\\r\\n    Count the number of alphanumeric characters in a given text.\\r\\n\\r\\n    Args:\\r\\n    text (str): The text to be analyzed.\\r\\n\\r\\n    Returns:\\r\\n    int: The total number of alphanumeric characters in the text.\\r\\n    \"\"\"\\r\\n    return sum(char.isalnum() for char in text)\\r\\n\\r\\ndef num_tokens_from_string(string: str) -> int:\\r\\n    \"\"\"Returns the number of tokens in a text string.\"\"\"\\r\\n    encoding = tiktoken.encoding_for_model(\"gpt-4-1106-preview\")\\r\\n    num_tokens = len(encoding.encode(string))\\r\\n    return num_tokens\\r\\n\\r\\ndef analyze_texts(texts):\\r\\n    \"\"\"\\r\\n    Analyze the given texts to count total, alphanumeric, and newline characters.\\r\\n\\r\\n    Args:\\r\\n    texts (dict): A dictionary where keys are identifiers (e.g., loader names) and\\r\\n                  values are the corresponding text strings.\\r\\n\\r\\n    Returns:\\r\\n    tuple of dicts: A tuple containing three dictionaries, each with counts of\\r\\n                    total characters, alphanumeric characters, and newline characters respectively.\\r\\n    \"\"\"\\r\\n    total_characters = {loader: len(text) for loader, text in texts.items()}\\r\\n    alphanumeric_characters = {loader: count_alphanumeric(text) for loader, text in texts.items()}\\r\\n    newline_characters = {loader: text.count(\\'\\\\n\\') for loader, text in texts.items()}\\r\\n    token_count = {loader: num_tokens_from_string(text) for loader, text in texts.items()}\\r\\n    return total_characters, alphanumeric_characters, newline_characters, token_count\\r\\n\\r\\ndef plot_data(data, title):\\r\\n    \"\"\"\\r\\n    Create a bar plot for the given data.\\r\\n\\r\\n    Args:\\r\\n    data (dict): A dictionary containing the data to be plotted. Keys are considered as labels\\r\\n                 and values as the corresponding data points.\\r\\n    title (str): The title of the plot.\\r\\n\\r\\n    Note:\\r\\n    The bars in the plot are colored blue, green, and red, in the order of the dictionary keys.\\r\\n    \"\"\"\\r\\n    plt.bar(data.keys(), data.values(), color=[\\'blue\\', \\'green\\', \\'red\\'])\\r\\n    plt.title(title)\\r\\n    plt.ylabel(\\'Count\\')\\r\\n    plt.xticks(rotation=45)\\r\\n    plt.show()\\r\\n\\r\\ntotal_chars, alphanumeric_chars, newline_chars, token_count = analyze_texts(texts)\\nAnalyzing the Extracted Text\\n\\nGraph by author\\nWebBaseLoader (16,191 Characters)\\nThe text includes a mix of the main article content, website navigation elements, metadata, and other peripheral information. This indicates that WebBaseLoader extracts all text from the HTML without differentiating between the main content and other page elements.\\nPotential Challenges for RAG\\xa0System\\n\\nNoise in Data: The presence of non-relevant text (e.g., menu items, footer information) can introduce noise, potentially impacting the accuracy and relevance of the RAG system‚Äôs outputs.\\nNeed for Post-Processing: To enhance the quality of embeddings, you might need to post-process this text to filter out irrelevant parts and focus on the main content.\\n\\nSeleniumURLLoader (23,598 Characters)\\n\\xa0The highest character count comes from the SeleniumURLLoader. This can be attributed to its method of loading pages as a browser would, capturing the primary content and potentially more of the surrounding elements and dynamically loaded content.\\nThis text, similar to the WebBaseLoader‚Äôs output, includes the main article content and additional elements like website headers, footers, and navigation links. However, it‚Äôs more focused on the article, suggesting a better capture of the intended content.\\nPotential Challenges for RAG\\xa0System\\n\\nReduced Noise, But Still Present: While there‚Äôs less irrelevant text compared to the WebBaseLoader output, the presence of some non-article elements can still introduce noise.\\nPost-Processing Consideration: Like with the WebBaseLoader, filtering out irrelevant parts will enhance the quality of embeddings.\\n\\nNewsURLLoader (7,580 Characters)\\nThe NewsURLLoader shows the lowest character count.\\nThe text appears more focused and streamlined than WebBaseLoader and SeleniumURLLoader‚Äôs outputs. It mainly consists of the main article content, with minimal peripheral information. This indicates that the NewsURLLoader is effectively targeting and extracting the core content of the news article.\\nPotential Challenges for RAG\\xa0System\\n\\nHigh Relevance and Quality: The content‚Äôs higher relevance and focused nature mean it‚Äôs more likely to produce accurate and contextually relevant embeddings in a vector database.\\nLimited Need for Post-Processing: Unlike the other two loaders, the NewsURLLoader requires minimal post-processing to filter out noise, as it already provides a clean extraction of the news content.\\n\\nImplications\\n\\nWebBaseLoader: Offers a balance between breadth and depth, suitable for general-purpose web scraping where capturing a wide range of content is necessary.\\nSeleniumURLLoader: Ideal for scenarios where comprehensive text capture, including dynamic content, is crucial. However, this can lead to a larger volume of data, potentially increasing processing time and resource usage.\\nNewsURLLoader: Best suited for applications where focused and relevant content extraction is key, such as news aggregation and analysis, providing clean and concise outputs.\\n\\nThese insights help in understanding how each loader functions and in choosing the right tool depending on the specific requirements of your application, especially in an RAG pipeline.\\nMore analysis\\nYou can do a similar analysis as above across different axes:\\nplot_data(alphanumeric_chars, \\'Number of Alphanumeric Characters\\')\\n\\nGraph by author\\nplot_data(newline_chars, \\'Number of Newline Characters\\')\\n\\nGraph by author\\nplot_data(token_count, \\'Number of Tokens\\')\\n\\nGraph by author\\nWhy do we see this difference?\\nThe discrepancy in the number of characters each loader extracted can be attributed to their distinct methodologies and the source code that drives their functionality.\\nHere‚Äôs a breakdown based on the source code and operational differences:\\nWebBaseLoader\\n\\nMethodology: It performs direct HTML fetching using HTTP requests and parses the HTML content with BeautifulSoup.\\nWhy the Difference: This loader extracts all text content from the HTML, including main content, navigation elements, headers, footers, and possibly some script elements. However, it does not execute JavaScript, so any content loaded dynamically (which is common in modern web pages) is not captured. This can lead to a moderate character count\\u200a‚Äî\\u200asubstantial but not exhaustive.\\n\\nSeleniumURLLoader\\n\\nMethodology: Uses Selenium for browser automation, which launches a browser instance (often in headless mode) and interacts with the page like a human user. It can execute JavaScript and capture dynamically loaded content.\\nWhy the Difference: The higher character count is likely due to this loader‚Äôs ability to capture more comprehensive content, including dynamic elements that only load upon user interaction or as a part of JavaScript execution. This method fetches the static HTML content and the additional text that becomes available as the page fully renders in a browser environment. This thorough approach results in capturing a larger volume of text.\\n\\nNewsURLLoader\\n\\nMethodology: Utilizes the newspaper library designed explicitly for scraping and curating news articles. It is optimized for extracting article content while excluding unrelated material.\\nWhy the Difference: The lower character count reflects its focused extraction. The newspaper library targets the core article text and is adept at ignoring extraneous content like ads, sidebars, or site navigation elements. This results in a cleaner and more concise text extraction, focusing primarily on the main news content.\\n\\nSummary\\n\\nWebBaseLoader: Provides a broad capture of HTML content but misses dynamic content, leading to a moderate character count.\\nSeleniumURLLoader: Captures a complete picture of the webpage, including dynamic content, which results in the highest character count.\\nNewsURLLoader: Highly specialized and focused on news content, leading to the lowest character count due to its targeted extraction.\\n\\nSplitting text for retrieval using RecursiveCharacterTextSplitter\\nRecursiveCharacterTextSplitter is designed to split text into chunks based on a list of separators, which can be tailored for different programming languages or text formats.\\nThe class employs a recursive approach to splitting, ensuring that if one separator doesn‚Äôt result in a split, it falls back to the next one in the list.\\nHere‚Äôs a breakdown of the key components and functionalities of this class:\\n\\nSeparators: The class takes a list of separators (separators) which are used to split the text. The default list includes common separators like new lines and spaces. The separators can be regular expressions if is_separator_regex is set to True.\\nRecursive Splitting: The method _split_text attempts to split the text using the provided separators. If a separator doesn‚Äôt successfully split the text, or if the resulting chunks are too large (exceed the specified chunk size), the method recursively tries with the next separator in the list.\\nLanguage-Specific Separators: The class can adapt its separators based on the programming language of the text, as indicated by the get_separators_for_language method. This method returns a list of separators appropriate for programming languages like Python, Java, C++, etc.\\nChunk Size and Merging: The class ensures that the resulting chunks are within a certain size limit (_chunk_size). If smaller chunks are created, they can be merged back together to ensure that each chunk is of a reasonable size.\\n\\nTo use this class effectively:\\n\\nInstantiate the Class: Create an instance of RecursiveCharacterTextSplitter, specify the separators if the default ones are unsuitable for your text.\\nSplit Texts: Use the split_text method to split the texts from each of your loaders.\\nPost-Processing: After splitting, you may need to post-process the chunks, especially if the splitting results in broken sentences or contexts.\\nFurther Analysis: Once the text is split into manageable chunks, you can proceed with your analysis by creating embeddings or pushing them to a vector database.\\n\\nThis class is handy when dealing with large text files or texts where a simple split by a single character (like a newline) is insufficient. It allows for a more nuanced and flexible approach to text splitting, catering to the specific structural nuances of different text types.\\ntext_splitter = RecursiveCharacterTextSplitter(\\r\\n    # Set a really small chunk size, just to show.\\r\\n    chunk_size = 250,\\r\\n    chunk_overlap  = 5,\\r\\n    length_function = len\\r\\n)\\r\\n\\r\\ntexts = {\\r\\n    \\'Web Base Loader\\': wb_loader_doc[0].page_content,\\r\\n    \\'Selenium Loader\\': selenium_loader_doc[0].page_content,\\r\\n    \\'News URL Loader\\': newsurl_docs[0].page_content\\r\\n}\\r\\n\\r\\ndef create_chunks(document):\\r\\n    return text_splitter.split_documents(document)\\r\\n\\r\\n# Creating chunks for each document\\r\\nwb_chunks = create_chunks(wb_loader_doc)\\r\\nselenium_chunks = create_chunks(selenium_loader_doc)\\r\\nnewsurl_chunks = create_chunks(newsurl_docs)\\r\\n\\r\\nchunk_counts = {\\r\\n    \\'WebBase Loader\\': len(wb_chunks),\\r\\n    \\'Selenium Loader\\': len(selenium_chunks),\\r\\n    \\'News URL Loader\\': len(newsurl_chunks)\\r\\n}\\r\\n\\r\\nplot_data(chunk_counts, \\'Number of Chunks in Each Document\\')\\n\\nGraph by author\\nRAG Pipeline\\nTo set up your RAG pipeline, you must create vector store retrievers. The following code will do that for you:\\ndef create_index_and_retriever(chunks, embeddings):\\r\\n    \"\"\"\\r\\n    Create an index and retriever for the given chunks using the specified embeddings.\\r\\n\\r\\n    Args:\\r\\n    chunks (list): List of text chunks to be indexed.\\r\\n    embeddings (Embeddings object): Embedding model used for creating the index.\\r\\n\\r\\n    Returns:\\r\\n    retriever (Retriever object): The retriever object for the created index.\\r\\n    \"\"\"\\r\\n    index = FAISS.from_documents(chunks, embeddings)\\r\\n    retriever = index.as_retriever()\\r\\n    return retriever\\r\\n\\r\\n\\r\\n# Embedding and Language Model setup\\r\\nembeddings = OpenAIEmbeddings(show_progress_bar=True)\\r\\nllm = ChatOpenAI(model=\"gpt-4-1106-preview\")\\r\\n\\r\\n# Creating indexes and retrievers\\r\\nwb_retriever = create_index_and_retriever(wb_chunks, embeddings)\\r\\nselenium_retriever = create_index_and_retriever(selenium_chunks, embeddings)\\r\\nnews_url_retriever = create_index_and_retriever(newsurl_chunks, embeddings)\\nThe following are the questions (queries )and ground truth answers (answers) that you‚Äôll use to assess the performance of each retriever.\\nqueries = [\\r\\n    \"What are educators\\' main concerns regarding using AI chatbots like ChatGPT by students?\",\\r\\n    \"Why do the Stanford researchers believe that concerns about AI chatbots leading to increased student cheating are misdirected?\",\\r\\n    \"What findings have the Stanford researchers gathered about the prevalence of cheating among U.S. high school students in the context of AI chatbots?\",\\r\\n    \"What alternative reasons might explain why students cheat, according to the article?\",\\r\\n    \"What recommendations or strategies do the article or researchers suggest for addressing academic dishonesty in schools?\"\\r\\n]\\r\\n\\r\\nanswers = [\\r\\n    \"Educators are concerned about students using AI chatbots like ChatGPT to cheat by passing off AI-generated writing as their own.\",\\r\\n    \"Stanford researchers believe concerns about AI chatbots leading to increased cheating are misdirected because cheating predates these technologies, and when students cheat, it\\'s typically for reasons unrelated to technology access.\",\\r\\n    \"Their research shows that 60% to 70% of students admitted to cheating before the advent of AI chatbots, and this rate has remained constant or even slightly decreased in 2023.\",\\r\\n    \"Alternative reasons for cheating include struggling with material, excessive homework, assignments feeling like busywork, and overwhelming pressure to achieve.\",\\r\\n    \"Recommended strategies include helping students feel more engaged and valued, addressing deeper systemic problems, and promoting a sense of belonging, purpose, and connection in the educational environment.\"\\r\\n]\\nThe QAChainRunner is a pivotal component designed to streamline the querying and retrieving answers using a RetrievalQA chain.\\nThis class, engineered for flexibility and efficiency, is a centralized conduit between the user‚Äôs queries and the complex machinery of language model-based retrieval systems. Upon initialization, it accepts a pre-defined language model (LLM), setting the stage for sophisticated query-processing operations.\\nIn action, the QAChainRunner takes a retriever object and a query as input.\\nIt then dynamically constructs a RetrievalQA chain, leveraging the power of the provided language model to interpret and process the query. The real strength of this class lies in its ability to handle multiple queries seamlessly, returning a structured and comprehensive set of results. Each result includes the original query, the generated answer, and the source documents that informed the response, offering an insightful peek into the retrieval process.\\nIn essence, QAChainRunner acts as an intelligent intermediary, transforming simple queries into insightful answers, making it an indispensable tool for any application or system focused on advanced information retrieval and question-answering tasks.\\nfrom typing import List, Dict, Any\\r\\nfrom datasets import Dataset\\r\\n\\r\\nclass QAChainRunner:\\r\\n    \"\"\"\\r\\n    Class to handle running queries through a RetrievalQA chain.\\r\\n    \"\"\"\\r\\n    def __init__(self, llm):\\r\\n        self.llm = llm\\r\\n\\r\\n    def run_retrieval_qa(self, retriever, query):\\r\\n        \"\"\"\\r\\n        Run a query through the RetrievalQA chain.\\r\\n\\r\\n        Args:\\r\\n        retriever (Retriever object): The retriever to use.\\r\\n        query (str): The query to process.\\r\\n\\r\\n        Returns:\\r\\n        dict: The response including the query, result, and source documents.\\r\\n        \"\"\"\\r\\n        try:\\r\\n            qa_chain = RetrievalQA.from_chain_type(llm=self.llm, \\r\\n                                                   retriever=retriever, \\r\\n                                                   verbose=True,\\r\\n                                                   return_source_documents=True)\\r\\n            return qa_chain.invoke(query)\\r\\n        except Exception as e:\\r\\n            print(f\"Error in running RetrievalQA: {e}\")\\r\\n            return {\"query\": query, \"result\": None, \"source_documents\": []}\\r\\n\\r\\n    def run_queries(self, retriever, queries: List[str]) -> List[Dict[str, Any]]:\\r\\n        \"\"\"\\r\\n        Run multiple queries through the RetrievalQA chain.\\r\\n\\r\\n        Args:\\r\\n        retriever (Retriever object): The retriever to use.\\r\\n        queries (List[str]): List of queries to process.\\r\\n\\r\\n        Returns:\\r\\n        List[Dict[str, Any]]: List of responses for each query.\\r\\n        \"\"\"\\r\\n        return [self.run_retrieval_qa(retriever, query) for query in queries]\\r\\n\\r\\ndef parse_retrieval_qa_results(results, ground_truths):\\r\\n    \"\"\"\\r\\n    Parse the results from the RetrievalQA pipeline into a structured format.\\r\\n\\r\\n    Args:\\r\\n    results (List[Dict[str, Any]]): Results from the RetrievalQA pipeline.\\r\\n    ground_truths (List[str]): Ground truth answers.\\r\\n\\r\\n    Returns:\\r\\n    Dict[str, List[Any]]: Parsed results including questions, answers, contexts, and ground truths.\\r\\n    \"\"\"\\r\\n    parsed_results = {\\'question\\': [], \\'answer\\': [], \\'contexts\\': [], \\'ground_truths\\': []}\\r\\n\\r\\n    for i, result in enumerate(results):\\r\\n        query = result.get(\\'query\\')\\r\\n        answer = result.get(\\'result\\')\\r\\n        source_documents = result.get(\\'source_documents\\', [])\\r\\n\\r\\n        # Transform Document objects into a compatible format (e.g., string or dict)\\r\\n        contexts = []\\r\\n        for doc in source_documents:\\r\\n            if hasattr(doc, \\'page_content\\'):\\r\\n                # Assuming doc is a Document object with a \\'page_content\\' attribute\\r\\n                contexts.append(doc.page_content)\\r\\n            elif isinstance(doc, dict):\\r\\n                # If doc is already a dictionary, use as is or convert to string\\r\\n                contexts.append(str(doc))\\r\\n            else:\\r\\n                # Fallback for other types\\r\\n                contexts.append(str(doc))\\r\\n\\r\\n        parsed_results[\\'question\\'].append(query)\\r\\n        parsed_results[\\'answer\\'].append(answer)\\r\\n        parsed_results[\\'contexts\\'].append(contexts)\\r\\n        parsed_results[\\'ground_truths\\'].append(ground_truths[i] if i < len(ground_truths) else [])\\r\\n\\r\\n    return parsed_results\\r\\n\\r\\n\\r\\ndef create_hf_dataset_from_dict(parsed_results: Dict[str, List[Any]]) -> Dataset:\\r\\n    \"\"\"\\r\\n    Convert parsed results into a Hugging Face Dataset object.\\r\\n\\r\\n    Args:\\r\\n    parsed_results (Dict[str, List[Any]]): Parsed results from the RetrievalQA pipeline.\\r\\n\\r\\n    Returns:\\r\\n    Dataset: A Hugging Face Dataset object.\\r\\n    \"\"\"\\r\\n    try:\\r\\n        return Dataset.from_dict(parsed_results)\\r\\n    except Exception as e:\\r\\n        print(f\"Error in creating dataset: {e}\")\\r\\n        return None\\nContext Recall and Context Precision in\\xa0ragas\\nContext Recall is a metric for information retrieval and natural language processing, particularly in systems like Retrieval-Augmented Generation (RAG). It measures the effectiveness of a retrieval system in fetching relevant information or documents that contribute meaningfully to generating accurate and contextually appropriate answers to queries.\\nContext Precision, similar to Context Recall, is a vital metric for evaluating information retrieval systems, particularly in contexts like Retrieval-Augmented Generation (RAG) models. While Context Recall focuses on the proportion of relevant documents retrieved from the total ground truths, Context Precision measures the relevance of the retrieved documents against all the documents retrieved.\\nHow Context Recall\\xa0Works\\n\\nInformation Retrieval: In RAG systems, when a query is posed, the model retrieves a set of documents or contexts that it believes are relevant to the query.\\nAnswer Generation: The model then uses these contexts and language understanding capabilities to generate an answer.\\n\\nWhat Context Recall\\xa0Measures\\n\\nRelevance of Retrieved Contexts: Context Recall assesses how many retrieved documents are relevant or helpful in answering the query.\\nEffectiveness of the Retrieval Component: It evaluates the retrieval component of the model, which is crucial for the overall quality of the answer.\\n\\nHow Context Recall is\\xa0Measured\\n\\nComparison With Ground Truths: Typically, for each query, there is a set of ground truth documents known to contain relevant information. Context Recall measures how many of these ground truth documents were retrieved by the model.\\nCalculation: It can be calculated as the proportion or count of relevant documents retrieved out of the total ground truth documents. This is often represented as a percentage or a ratio.\\n\\nImportance in AI\\xa0Models\\n\\nImproves Answer Quality: High Context Recall indicates that the model effectively fetches relevant information, which is crucial for generating accurate and comprehensive answers.\\nModel Optimization: By measuring Context Recall, developers can fine-tune the retrieval component of the model for better performance.\\n\\nHow Context Precision Works\\n\\nInformation Retrieval: When a query is posed in a RAG system, the system retrieves a set of documents or contexts.\\nRelevance Assessment: Context Precision assesses how many of these retrieved documents are relevant to the query.\\n\\nWhat Context Precision Measures\\n\\nAccuracy of Retrieved Contexts: It measures the proportion of the retrieved documents relevant to the query.\\nEfficiency of the Retrieval Component: High Context Precision indicates that the model‚Äôs retrieval component is active and accurate, fetching more relevant documents than irrelevant ones.\\n\\nHow Context Precision is\\xa0Measured\\n\\nComparison With Relevant Documents: Context Precision is calculated by dividing the number of relevant documents retrieved by the total number of documents retrieved for each query.\\nCalculation: Often expressed as a percentage, it indicates the retrieval system‚Äôs accuracy.\\n\\nImportance in AI\\xa0Models\\n\\nEnhances the Quality of Generated Answers: Context Precision helps generate more accurate and contextually correct answers by ensuring that the retrieved documents are primarily relevant.\\nModel Optimization and Balancing: Alongside Context Recall, Context Precision helps fine-tune RAG models‚Äô retrieval components. A balance between Context Recall and Precision is often sought for optimal performance.\\n\\nfrom ragas import evaluate\\r\\n\\r\\nfrom ragas.metrics import context_recall, context_precision\\r\\n\\r\\nwb_result = evaluate(wb_dataset, metrics=[context_precision, context_recall])\\r\\n\\r\\nselenium_result = evaluate(selenium_dataset, metrics=[context_precision, context_recall])\\r\\n\\r\\nnewsurl_result = evaluate(newsurl_dataset, metrics=[context_precision, context_recall])\\nInterpretation of\\xa0Results\\nI‚Äôm not gonna lie: I am pretty surprised by the results. I was expecting to see the NewsURLLoader win across all metrics.\\n\\nGraph by author\\nWeb Base Loader‚Äôs Superiority: The Web Base loader has the highest RAGAS score, indicating it‚Äôs the most effective overall in Retrieval Augmented Generation. Its high context precision and recall suggest it‚Äôs adept at retrieving relevant documents without missing many important ones.\\nSelenium Loader‚Äôs Balanced Performance: The Selenium loader shows a slightly lower RAGAS score but maintains a high context recall, equal to the Web Base loader. Its context precision is lower, though, which might suggest it retrieves more documents, but a slightly larger proportion of them are less relevant.\\nNews URL Loader‚Äôs Lower Recall: While matching the Web Base loader in precision, the News URL loader falls behind in context recall and RAGAS score. This could indicate that while it‚Äôs good at finding relevant documents, it misses many relevant ones compared to the other loaders.\\nThe observation that the NewsURLLoader extracts cleaner text yet performs lower in terms of the overall RAGAS score and context recall is quite intriguing and points to a few potential reasons:\\nPrecision vs. Quality of Content: While the NewsURLLoader might be retrieving cleaner, more precise text, the effectiveness of a retrieval system in a Retrieval-Augmented Generation (RAG) setup is not solely determined by text cleanliness. The key is to retrieve content that is not just clean but also highly relevant and comprehensive in answering the query. If the cleaner text is less comprehensive or slightly off-topic, it might contribute less effectively to the answer generation, impacting the RAGAS score.\\nNature of Source Documents: The NewsURLLoader might be optimized for extracting text from news websites, which often have cleaner and more structured content. However, if the content from these sources is less diverse and rich in answering a wide array of queries compared to other sources, it might lead to lower recall and RAGAS scores.\\nContext Recall Challenge: The lower context recall score suggests that the NewsURLLoader, despite retrieving high-quality text, might be missing out on a significant number of relevant documents. This could be due to stricter or more conservative retrieval algorithms, which prefer precision over the breadth of retrieval.\\nMatching Query with Context: The effectiveness of a RAG system also depends on how well the retrieved context aligns with the nuances of the query. If the NewsURLLoader‚Äôs algorithm is tuned to favour text cleanliness over nuanced matching, it might retrieve text that, while clean, is not as aligned with the specific needs of the query.\\nIntegration with the RAG System: The overall architecture and integration of the NewsURLLoader with the RAG system could also play a role. Even if the text is cleaner, other aspects, like how the loader interfaces with the language model, the handling of metadata, and the overall synergy with the RAG process, are crucial.\\n\\n\\n\\n\\nLangChainLanguage ModelsLLMLLMOpsPrompt Engineering\\n\\n\\n\\n \\n\\nHarpreet Sahota\\n\\n\\n\\n\\nRelated Articles\\n\\n\\n\\n\\n\\n \\n\\n\\n\\nBuilding a Low-Cost Local LLM Server to Run 70 Billion Parameter Models\\n\\n\\n\\nFabr√≠cio Ceolin \\n\\n                                August 30, 2024                            \\n\\nA guest post from Fabr√≠cio Ceolin, DevOps Engineer at Comet. Inspired by the growing demand‚Ä¶\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\nHow to Evaluate Your RAG Using the RAGAs Framework\\n\\n\\n\\nAlexandru Razvant and Decoding ML \\n\\n                                July 31, 2024                            \\n\\nWelcome to Lesson 10 of 11 in our free course series, LLM Twin: Building Your‚Ä¶\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\nArchitect Scalable and Cost-Effective LLM & RAG Inference Pipelines\\n\\n\\n\\nPaul Iusztin and Decoding ML \\n\\n                                July 23, 2024                            \\n\\nWelcome to Lesson 9 of 11 in our free course series, LLM Twin: Building Your‚Ä¶\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSubscribe to Comet\\n\\n\\nSign up to receive the Comet newsletter and never miss out on the latest ML updates, news and events!\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nThank you for subscribing to Comet‚Äôs newsletter!\\n\\n\\nProductsExperiment Management\\nArtifacts\\nModel Registry\\nModel Production Monitoring\\nLLMOps\\nLearnDocumentation\\nResources\\nComet Blog\\nDeep Learning Weekly\\nHeartbeat\\nLLM Course\\nCompanyAbout Us\\nNews and Events\\nCareers\\nContact Us\\nPricingPricing\\nCreate a Free Account\\nContact Sales\\nFollow UsLinkedInTwitterYoutubeFacebookLearnDocumentation\\nResources\\nComet Blog\\nDeep Learning Weekly\\nHeartbeat\\nLLM Course\\nProductsExperiment Management\\nArtifacts\\nModel Registry\\nModel Production Monitoring\\nLLMOps\\nCompanyAbout Us\\nNews and Events\\nCareers\\nContact Us\\nPricingPricing\\nCreate a Free Account\\nContact Sales\\nFollow UsLinkedInTwitterYoutubeFacebook \\n\\n\\n\\n¬© 2024  Comet ML, Inc. - All Rights Reserved\\nTerms of Service\\nPrivacy Policy\\nCCPA Privacy Notice\\nCookie Settings\\n\\n\\n\\n\\nEnterprise\\nProducts\\n\\nExperiment Management\\nArtifacts\\nModel Registry\\nModel Production Monitoring\\nLLMOps\\n\\n\\nDocs\\nPricing\\nCustomers\\nLearn\\n\\nResources\\nBlog\\n\\n\\nCompany\\n\\nAbout Us\\nNews and Events\\n\\nEvents\\nPress Releases\\n\\n\\nCareers\\nLeadership\\nContact Us\\n\\n\\nLogin\\nGet Demo\\nTry Comet Free\\n\\nBack To Top\\n\\n\\n\\n\\n\\n\\n\\n\\nWe use cookies to collect statistical usage information about our website and its visitors and ensure we give you the best experience on our website. Please refer to our Privacy Policy to learn more.OkPrivacy policy\\n\\n\\n\\n')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter #split the text into documents \n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 200)\n",
    "documents = text_splitter.split_documents(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://www.comet.com/site/blog/langchain-document-loaders-for-web-data/', 'title': 'LangChain Document Loaders for Web\\xa0Data - Comet', 'description': 'The effectiveness of RAG hinges on the method used to retrieve documents. Explore 3 key LangChain document loaders + how they effect output', 'language': 'en-US'}, page_content='LangChain Document Loaders for Web\\xa0Data - Comet\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n \\n\\n\\n\\n\\n\\nskip to Main Content\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nEnterprise\\nProducts\\n\\nExperiment Management\\nArtifacts\\nModel Registry\\nModel Production Monitoring\\nLLMOps\\n\\n\\nDocs\\nPricing\\nCustomers\\nLearn\\n\\nResources\\nBlog\\nDeep Learning Weekly\\nLLM Course\\n\\n\\nCompany\\n\\nAbout Us\\nNews and Events\\n\\nEvents\\nPress Releases\\n\\n\\nCareers\\nContact Us\\nLeadership\\n\\n\\nLogin\\nGet Demo\\nTry Comet Free\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\nSearch\\n\\n\\nSubmit\\n\\n\\nHome ‚Ä∫ Blog ‚Ä∫ LangChain Document Loaders for Web\\xa0Data\\n\\n\\n\\n\\n\\n\\nLangChain Document Loaders for Web\\xa0Data\\n\\n\\n\\nWords By \\nHarpreet Sahota \\n\\n            November 30, 2023        \\n\\n\\n\\n\\n\\nAnd An Assessment of How They Impact Your ragas Metrics'),\n",
       " Document(metadata={'source': 'https://www.comet.com/site/blog/langchain-document-loaders-for-web-data/', 'title': 'LangChain Document Loaders for Web\\xa0Data - Comet', 'description': 'The effectiveness of RAG hinges on the method used to retrieve documents. Explore 3 key LangChain document loaders + how they effect output', 'language': 'en-US'}, page_content='LangChain Document Loaders for Web\\xa0Data\\n\\n\\n\\nWords By \\nHarpreet Sahota \\n\\n            November 30, 2023        \\n\\n\\n\\n\\n\\nAnd An Assessment of How They Impact Your ragas Metrics\\n\\nPhoto by Ilya Pavlov on\\xa0Unsplash\\nIf you‚Äôve ever wondered how the quality of information sourced by language models affects their outputs, you‚Äôre in the right place.\\xa0I‚Äôm trying to unpack how different document loaders in LangChain impact a Retrieval Augmented Generation (RAG) system.\\nWhy is this important?\\xa0\\nRAG is a game-changer. It cleverly combines retrieving information from external documents with the generative capabilities of language models. However, the effectiveness of this system hinges on one critical aspect\\u200a‚Äî\\u200athe method used to retrieve documents.\\nThis blog is about exploring and understanding this pivotal element.\\nWe‚Äôll focus on three key players in LangChain:\\n\\nWebBaseLoader\\nSeleniumURLLoader,\\nNewsURLLoader.'),\n",
       " Document(metadata={'source': 'https://www.comet.com/site/blog/langchain-document-loaders-for-web-data/', 'title': 'LangChain Document Loaders for Web\\xa0Data - Comet', 'description': 'The effectiveness of RAG hinges on the method used to retrieve documents. Explore 3 key LangChain document loaders + how they effect output', 'language': 'en-US'}, page_content='Each has its approach to fetching information, and we will find out how these methods shape the final output of RAG models.\\nI invite you to join this exploration\\u200a‚Äî\\u200ait‚Äôs not just an exploration of code and algorithms but a journey to enhance the intelligence and responsiveness of AI systems.\\nüßëüèΩ\\u200düíª Let‚Äôs write some\\xa0code!\\nStart with some preliminaries and setting the environment.\\n%%capture\\r\\n!pip install langchain openai unstructured selenium newspaper3k textstat tiktoken faiss-cpu\\r\\n\\r\\nimport os\\r\\nimport getpass\\r\\nfrom langchain.document_loaders import WebBaseLoader, UnstructuredURLLoader, NewsURLLoader, SeleniumURLLoader\\r\\n\\r\\nimport tiktoken\\r\\nimport matplotlib.pyplot as plt\\r\\nimport pandas as pd\\r\\nimport nltk\\r\\nfrom nltk.tokenize import sent_tokenize, word_tokenize\\r\\nfrom nltk.corpus import stopwords\\r\\nfrom textstat import flesch_reading_ease\\r\\nfrom collections import Counter\\r\\n\\r\\nfrom langchain.embeddings.openai import OpenAIEmbeddings\\r\\nfrom langchain.vectorstores import FAISS'),\n",
       " Document(metadata={'source': 'https://www.comet.com/site/blog/langchain-document-loaders-for-web-data/', 'title': 'LangChain Document Loaders for Web\\xa0Data - Comet', 'description': 'The effectiveness of RAG hinges on the method used to retrieve documents. Explore 3 key LangChain document loaders + how they effect output', 'language': 'en-US'}, page_content='from textstat import flesch_reading_ease\\r\\nfrom collections import Counter\\r\\n\\r\\nfrom langchain.embeddings.openai import OpenAIEmbeddings\\r\\nfrom langchain.vectorstores import FAISS\\r\\nfrom langchain.chat_models import ChatOpenAI\\r\\nfrom langchain.chains import RetrievalQA\\r\\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\\r\\nos.environ[\\'OPENAI_API_KEY\\'] = getpass.getpass(\"Input your Open AI Key:\")\\nFor this demonstration, we‚Äôll use this website.\\nwebsite = \"https://phys.org/news/2023-11-qa-dont-blame-chatbots.html\"\\nThe function below will load the website into a LangChain document object:\\ndef load_document(loader_class, website_url):\\r\\n    \"\"\"\\r\\n    Load a document using the specified loader class and website URL.\\r\\n\\r\\n    Args:\\r\\n    loader_class (class): The class of the loader to be used.\\r\\n    website_url (str): The URL of the website from which to load the document.\\r\\n\\r\\n    Returns:\\r\\n    str: The loaded document.\\r\\n    \"\"\"\\r\\n    loader = loader_class([website_url])'),\n",
       " Document(metadata={'source': 'https://www.comet.com/site/blog/langchain-document-loaders-for-web-data/', 'title': 'LangChain Document Loaders for Web\\xa0Data - Comet', 'description': 'The effectiveness of RAG hinges on the method used to retrieve documents. Explore 3 key LangChain document loaders + how they effect output', 'language': 'en-US'}, page_content='website_url (str): The URL of the website from which to load the document.\\r\\n\\r\\n    Returns:\\r\\n    str: The loaded document.\\r\\n    \"\"\"\\r\\n    loader = loader_class([website_url])\\r\\n    return loader.load()\\nUnderstanding the WebBaseLoader'),\n",
       " Document(metadata={'source': 'https://www.comet.com/site/blog/langchain-document-loaders-for-web-data/', 'title': 'LangChain Document Loaders for Web\\xa0Data - Comet', 'description': 'The effectiveness of RAG hinges on the method used to retrieve documents. Explore 3 key LangChain document loaders + how they effect output', 'language': 'en-US'}, page_content='Photo by Emile Perron on\\xa0Unsplash\\nWhen extracting text from websites, the WebBaseLoader in LangChain is a tool you need to know about.\\nIt‚Äôs like a skilled miner adept at digging through the layers of a website to retrieve the valuable textual content beneath. Let‚Äôs explain exactly how it works and what this means for embedding documents into a vector database.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nWant to learn how to build modern software with LLMs using the newest tools and techniques in the field? Check out this free LLMOps course from industry expert Elvis Saravia of\\xa0DAIR.AI!'),\n",
       " Document(metadata={'source': 'https://www.comet.com/site/blog/langchain-document-loaders-for-web-data/', 'title': 'LangChain Document Loaders for Web\\xa0Data - Comet', 'description': 'The effectiveness of RAG hinges on the method used to retrieve documents. Explore 3 key LangChain document loaders + how they effect output', 'language': 'en-US'}, page_content='Want to learn how to build modern software with LLMs using the newest tools and techniques in the field? Check out this free LLMOps course from industry expert Elvis Saravia of\\xa0DAIR.AI!\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHow WebBaseLoader Retrieves Text\\nThe WebBaseLoader uses HTTP requests, a basic yet powerful way to communicate with web servers. Think of it as sending a letter to a website asking for its content. Once the website replies, WebBaseLoader takes over, sifting through the HTML\\u200a‚Äî\\u200athe foundational code of web pages.\\nThis is where BeautifulSoup, a Python library, comes into play. WebBaseLoader uses BeautifulSoup to parse the HTML, effectively reading and extracting the text. It‚Äôs like having a translator who can interpret the complex language of HTML and present you with just the readable text.\\nImpact on Document Embedding and Vector Databases\\nWhen this extracted text is embedded into a vector database, there are a few implications:'),\n",
       " Document(metadata={'source': 'https://www.comet.com/site/blog/langchain-document-loaders-for-web-data/', 'title': 'LangChain Document Loaders for Web\\xa0Data - Comet', 'description': 'The effectiveness of RAG hinges on the method used to retrieve documents. Explore 3 key LangChain document loaders + how they effect output', 'language': 'en-US'}, page_content='Quality of Extracted Text: WebBaseLoader relies on HTML structure and excels with well-structured websites. However, it might struggle with JavaScript-generated dynamic content, which is increasingly common in modern web design. This means the text it retrieves is as good as the HTML it interprets.\\nEfficiency: WebBaseLoader is efficient and fast, handling multiple requests seamlessly. This efficiency translates into quicker embedding of documents into your vector database, which is crucial for large-scale applications.\\nRelevance: The relevance of the extracted text can vary. In cases where websites are loaded with ads or unrelated content alongside the main text, WebBaseLoader might fetch some noise and valuable data. This could impact the precision of your RAG system‚Äôs outputs.'),\n",
       " Document(metadata={'source': 'https://www.comet.com/site/blog/langchain-document-loaders-for-web-data/', 'title': 'LangChain Document Loaders for Web\\xa0Data - Comet', 'description': 'The effectiveness of RAG hinges on the method used to retrieve documents. Explore 3 key LangChain document loaders + how they effect output', 'language': 'en-US'}, page_content='While it‚Äôs efficient and effective for static content, its performance can be limited by dynamic web elements. Remember the content you‚Äôre targeting as we dive into document embedding and vector databases.\\nIf you focus on static, well-structured websites, WebBaseLoader could be your workhorse in the RAG pipeline.\\nwb_loader_doc = load_document(WebBaseLoader, website)\\n\\xa0You can examine the extracted content from any of the loaders with the following pattern:\\nwb_loader_doc[0].page_content\\nGrasping the SeleniumURLLoader'),\n",
       " Document(metadata={'source': 'https://www.comet.com/site/blog/langchain-document-loaders-for-web-data/', 'title': 'LangChain Document Loaders for Web\\xa0Data - Comet', 'description': 'The effectiveness of RAG hinges on the method used to retrieve documents. Explore 3 key LangChain document loaders + how they effect output', 'language': 'en-US'}, page_content='Photo by Markus Spiske on\\xa0Unsplash\\nImagine a scenario where you need to extract text from a website as dynamic as a bustling city street\\u200a‚Äî\\u200achanging every moment, filled with interactive elements and content that loads as you scroll.\\nThe SeleniumURLLoader steps in, bringing a different skill set than the WebBaseLoader.\\nHow SeleniumURLLoader Retrieves Text\\nThe SeleniumURLLoader is like an undercover agent in the world of web browsers.\\nIt doesn‚Äôt just send a request to a website; it navigates the web as a user would. Using Selenium, a powerful tool for browser automation, it opens an actual browser window (in headless mode, meaning without a graphical interface) and interacts with the webpage. This ability to simulate user interactions is crucial for websites where content is rendered through JavaScript\\u200a‚Äî\\u200aa common scenario in modern web development.\\nBefore extracting the text, the loader waits for the page to load, including any dynamic content.'),\n",
       " Document(metadata={'source': 'https://www.comet.com/site/blog/langchain-document-loaders-for-web-data/', 'title': 'LangChain Document Loaders for Web\\xa0Data - Comet', 'description': 'The effectiveness of RAG hinges on the method used to retrieve documents. Explore 3 key LangChain document loaders + how they effect output', 'language': 'en-US'}, page_content='Before extracting the text, the loader waits for the page to load, including any dynamic content.\\nImpact on Document Embedding and Vector Databases'),\n",
       " Document(metadata={'source': 'https://www.comet.com/site/blog/langchain-document-loaders-for-web-data/', 'title': 'LangChain Document Loaders for Web\\xa0Data - Comet', 'description': 'The effectiveness of RAG hinges on the method used to retrieve documents. Explore 3 key LangChain document loaders + how they effect output', 'language': 'en-US'}, page_content='Comprehensive Text Retrieval: Since it interacts with web pages like a human user, SeleniumURLLoader can retrieve text that other methods might miss. This includes content that appears due to user interactions or is dynamically loaded by JavaScript.\\nPerformance Considerations: The thoroughness of SeleniumURLLoader comes at a cost. It‚Äôs slower and more resource-intensive than more straightforward HTTP request methods. When embedding documents into a vector database, this could mean longer processing times, especially for large volumes of data.\\nAccuracy and Relevance: The text retrieved by SeleniumURLLoader tends to be highly accurate and reflective of the user‚Äôs experience on the website. This can lead to more relevant and context-rich embeddings in your vector database, potentially enhancing the quality of your RAG system‚Äôs outputs.'),\n",
       " Document(metadata={'source': 'https://www.comet.com/site/blog/langchain-document-loaders-for-web-data/', 'title': 'LangChain Document Loaders for Web\\xa0Data - Comet', 'description': 'The effectiveness of RAG hinges on the method used to retrieve documents. Explore 3 key LangChain document loaders + how they effect output', 'language': 'en-US'}, page_content='The SeleniumURLLoader is your toolkit‚Äôs Swiss army knife for dealing with dynamic, JavaScript-heavy websites. It offers a depth of text retrieval unmatched by more straightforward methods but requires more resources and time.\\nA RAG pipeline is the ideal choice when your focus is on comprehensively capturing the essence of modern, interactive web pages.\\nselenium_loader_doc = load_document(SeleniumURLLoader, website)\\nWith the WebBaseLoader and SeleniumURLLoader covered, we‚Äôll next explore the NewsURLLoader, a specialized tool for news content.\\nUnveiling the NewsURLLoader in LangChain\\nNewsURLLoader is designed specifically for news articles.\\nHow NewsURLLoader Retrieves Text\\nThe NewsURLLoader doesn‚Äôt just fetch text; it‚Äôs adept at navigating through the unique structure of news articles.'),\n",
       " Document(metadata={'source': 'https://www.comet.com/site/blog/langchain-document-loaders-for-web-data/', 'title': 'LangChain Document Loaders for Web\\xa0Data - Comet', 'description': 'The effectiveness of RAG hinges on the method used to retrieve documents. Explore 3 key LangChain document loaders + how they effect output', 'language': 'en-US'}, page_content='How NewsURLLoader Retrieves Text\\nThe NewsURLLoader doesn‚Äôt just fetch text; it‚Äôs adept at navigating through the unique structure of news articles.\\nUsing the newspaper library, a Python package tailored for news extraction, performs a more refined retrieval. This loader not only fetches the article but also understands the typical layout of news websites, effectively separating the main content from the clutter of ads and sidebars. Moreover, the NewsURLLoader can perform light NLP (Natural Language Processing) tasks.\\nThis means it doesn‚Äôt just hand you the text; it can also provide summaries and extract keywords, offering a more concise and focused insight into the content.\\nImpact on Document Embedding and Vector Databases'),\n",
       " Document(metadata={'source': 'https://www.comet.com/site/blog/langchain-document-loaders-for-web-data/', 'title': 'LangChain Document Loaders for Web\\xa0Data - Comet', 'description': 'The effectiveness of RAG hinges on the method used to retrieve documents. Explore 3 key LangChain document loaders + how they effect output', 'language': 'en-US'}, page_content='Targeted and Clean Extraction: The NewsURLLoader is designed explicitly for news content, which means it can efficiently extract clean and relevant text from news articles. This leads to high-quality document embeddings, especially valuable for news-related queries in an RAG system.\\nNLP Enhancements: The optional NLP features of the NewsURLLoader add an extra layer of value. By embedding summarized content and key terms, your vector database can become more efficient, focusing on the essence rather than the bulk of news articles.\\nScope Limitation: While it‚Äôs a powerhouse for news content, the NewsURLLoader‚Äôs specialization is also its limitation. It‚Äôs different than the tool for general-purpose web scraping or for handling dynamic, interactive content like the SeleniumURLLoader.'),\n",
       " Document(metadata={'source': 'https://www.comet.com/site/blog/langchain-document-loaders-for-web-data/', 'title': 'LangChain Document Loaders for Web\\xa0Data - Comet', 'description': 'The effectiveness of RAG hinges on the method used to retrieve documents. Explore 3 key LangChain document loaders + how they effect output', 'language': 'en-US'}, page_content='The NewsURLLoader shines in its domain, making it an excellent choice for RAG systems focused on current events, journalism, or news analysis. It offers clean, concise, and relevant text extraction, with the bonus of NLP processing.\\nAnalyzing the content from each\\xa0loader\\nIn this analysis, you‚Äôll dive deep into the text extracted by the three document loaders: WebBaseLoader, SeleniumURLLoader, and NewsURLLoader.\\nYou‚Äôll compare their outputs based on specific metrics: the total number of characters, the count of alphanumeric characters, the number of newline characters, and the total number of tokens as determined by GPT-4 encoding.\\nThe goal is to quantitatively assess the nature and quality of text each loader extracts. This technical analysis will provide clear insights into the efficiency and accuracy of these loaders, helping us understand their impact on a Retrieval Augmented Generation system.'),\n",
       " Document(metadata={'source': 'https://www.comet.com/site/blog/langchain-document-loaders-for-web-data/', 'title': 'LangChain Document Loaders for Web\\xa0Data - Comet', 'description': 'The effectiveness of RAG hinges on the method used to retrieve documents. Explore 3 key LangChain document loaders + how they effect output', 'language': 'en-US'}, page_content='You‚Äôll present our findings through concise bar plots, comparing each loader‚Äôs performance straightforwardly.\\ndef count_alphanumeric(text):\\r\\n    \"\"\"\\r\\n    Count the number of alphanumeric characters in a given text.\\r\\n\\r\\n    Args:\\r\\n    text (str): The text to be analyzed.\\r\\n\\r\\n    Returns:\\r\\n    int: The total number of alphanumeric characters in the text.\\r\\n    \"\"\"\\r\\n    return sum(char.isalnum() for char in text)\\r\\n\\r\\ndef num_tokens_from_string(string: str) -> int:\\r\\n    \"\"\"Returns the number of tokens in a text string.\"\"\"\\r\\n    encoding = tiktoken.encoding_for_model(\"gpt-4-1106-preview\")\\r\\n    num_tokens = len(encoding.encode(string))\\r\\n    return num_tokens\\r\\n\\r\\ndef analyze_texts(texts):\\r\\n    \"\"\"\\r\\n    Analyze the given texts to count total, alphanumeric, and newline characters.\\r\\n\\r\\n    Args:\\r\\n    texts (dict): A dictionary where keys are identifiers (e.g., loader names) and\\r\\n                  values are the corresponding text strings.\\r\\n\\r\\n    Returns:'),\n",
       " Document(metadata={'source': 'https://www.comet.com/site/blog/langchain-document-loaders-for-web-data/', 'title': 'LangChain Document Loaders for Web\\xa0Data - Comet', 'description': 'The effectiveness of RAG hinges on the method used to retrieve documents. Explore 3 key LangChain document loaders + how they effect output', 'language': 'en-US'}, page_content='Args:\\r\\n    texts (dict): A dictionary where keys are identifiers (e.g., loader names) and\\r\\n                  values are the corresponding text strings.\\r\\n\\r\\n    Returns:\\r\\n    tuple of dicts: A tuple containing three dictionaries, each with counts of\\r\\n                    total characters, alphanumeric characters, and newline characters respectively.\\r\\n    \"\"\"\\r\\n    total_characters = {loader: len(text) for loader, text in texts.items()}\\r\\n    alphanumeric_characters = {loader: count_alphanumeric(text) for loader, text in texts.items()}\\r\\n    newline_characters = {loader: text.count(\\'\\\\n\\') for loader, text in texts.items()}\\r\\n    token_count = {loader: num_tokens_from_string(text) for loader, text in texts.items()}\\r\\n    return total_characters, alphanumeric_characters, newline_characters, token_count\\r\\n\\r\\ndef plot_data(data, title):\\r\\n    \"\"\"\\r\\n    Create a bar plot for the given data.\\r\\n\\r\\n    Args:'),\n",
       " Document(metadata={'source': 'https://www.comet.com/site/blog/langchain-document-loaders-for-web-data/', 'title': 'LangChain Document Loaders for Web\\xa0Data - Comet', 'description': 'The effectiveness of RAG hinges on the method used to retrieve documents. Explore 3 key LangChain document loaders + how they effect output', 'language': 'en-US'}, page_content='return total_characters, alphanumeric_characters, newline_characters, token_count\\r\\n\\r\\ndef plot_data(data, title):\\r\\n    \"\"\"\\r\\n    Create a bar plot for the given data.\\r\\n\\r\\n    Args:\\r\\n    data (dict): A dictionary containing the data to be plotted. Keys are considered as labels\\r\\n                 and values as the corresponding data points.\\r\\n    title (str): The title of the plot.\\r\\n\\r\\n    Note:\\r\\n    The bars in the plot are colored blue, green, and red, in the order of the dictionary keys.\\r\\n    \"\"\"\\r\\n    plt.bar(data.keys(), data.values(), color=[\\'blue\\', \\'green\\', \\'red\\'])\\r\\n    plt.title(title)\\r\\n    plt.ylabel(\\'Count\\')\\r\\n    plt.xticks(rotation=45)\\r\\n    plt.show()\\r\\n\\r\\ntotal_chars, alphanumeric_chars, newline_chars, token_count = analyze_texts(texts)\\nAnalyzing the Extracted Text'),\n",
       " Document(metadata={'source': 'https://www.comet.com/site/blog/langchain-document-loaders-for-web-data/', 'title': 'LangChain Document Loaders for Web\\xa0Data - Comet', 'description': 'The effectiveness of RAG hinges on the method used to retrieve documents. Explore 3 key LangChain document loaders + how they effect output', 'language': 'en-US'}, page_content='Graph by author\\nWebBaseLoader (16,191 Characters)\\nThe text includes a mix of the main article content, website navigation elements, metadata, and other peripheral information. This indicates that WebBaseLoader extracts all text from the HTML without differentiating between the main content and other page elements.\\nPotential Challenges for RAG\\xa0System\\n\\nNoise in Data: The presence of non-relevant text (e.g., menu items, footer information) can introduce noise, potentially impacting the accuracy and relevance of the RAG system‚Äôs outputs.\\nNeed for Post-Processing: To enhance the quality of embeddings, you might need to post-process this text to filter out irrelevant parts and focus on the main content.'),\n",
       " Document(metadata={'source': 'https://www.comet.com/site/blog/langchain-document-loaders-for-web-data/', 'title': 'LangChain Document Loaders for Web\\xa0Data - Comet', 'description': 'The effectiveness of RAG hinges on the method used to retrieve documents. Explore 3 key LangChain document loaders + how they effect output', 'language': 'en-US'}, page_content='SeleniumURLLoader (23,598 Characters)\\n\\xa0The highest character count comes from the SeleniumURLLoader. This can be attributed to its method of loading pages as a browser would, capturing the primary content and potentially more of the surrounding elements and dynamically loaded content.\\nThis text, similar to the WebBaseLoader‚Äôs output, includes the main article content and additional elements like website headers, footers, and navigation links. However, it‚Äôs more focused on the article, suggesting a better capture of the intended content.\\nPotential Challenges for RAG\\xa0System\\n\\nReduced Noise, But Still Present: While there‚Äôs less irrelevant text compared to the WebBaseLoader output, the presence of some non-article elements can still introduce noise.\\nPost-Processing Consideration: Like with the WebBaseLoader, filtering out irrelevant parts will enhance the quality of embeddings.'),\n",
       " Document(metadata={'source': 'https://www.comet.com/site/blog/langchain-document-loaders-for-web-data/', 'title': 'LangChain Document Loaders for Web\\xa0Data - Comet', 'description': 'The effectiveness of RAG hinges on the method used to retrieve documents. Explore 3 key LangChain document loaders + how they effect output', 'language': 'en-US'}, page_content='NewsURLLoader (7,580 Characters)\\nThe NewsURLLoader shows the lowest character count.\\nThe text appears more focused and streamlined than WebBaseLoader and SeleniumURLLoader‚Äôs outputs. It mainly consists of the main article content, with minimal peripheral information. This indicates that the NewsURLLoader is effectively targeting and extracting the core content of the news article.\\nPotential Challenges for RAG\\xa0System\\n\\nHigh Relevance and Quality: The content‚Äôs higher relevance and focused nature mean it‚Äôs more likely to produce accurate and contextually relevant embeddings in a vector database.\\nLimited Need for Post-Processing: Unlike the other two loaders, the NewsURLLoader requires minimal post-processing to filter out noise, as it already provides a clean extraction of the news content.\\n\\nImplications'),\n",
       " Document(metadata={'source': 'https://www.comet.com/site/blog/langchain-document-loaders-for-web-data/', 'title': 'LangChain Document Loaders for Web\\xa0Data - Comet', 'description': 'The effectiveness of RAG hinges on the method used to retrieve documents. Explore 3 key LangChain document loaders + how they effect output', 'language': 'en-US'}, page_content=\"Implications\\n\\nWebBaseLoader: Offers a balance between breadth and depth, suitable for general-purpose web scraping where capturing a wide range of content is necessary.\\nSeleniumURLLoader: Ideal for scenarios where comprehensive text capture, including dynamic content, is crucial. However, this can lead to a larger volume of data, potentially increasing processing time and resource usage.\\nNewsURLLoader: Best suited for applications where focused and relevant content extraction is key, such as news aggregation and analysis, providing clean and concise outputs.\\n\\nThese insights help in understanding how each loader functions and in choosing the right tool depending on the specific requirements of your application, especially in an RAG pipeline.\\nMore analysis\\nYou can do a similar analysis as above across different axes:\\nplot_data(alphanumeric_chars, 'Number of Alphanumeric Characters')\\n\\nGraph by author\\nplot_data(newline_chars, 'Number of Newline Characters')\"),\n",
       " Document(metadata={'source': 'https://www.comet.com/site/blog/langchain-document-loaders-for-web-data/', 'title': 'LangChain Document Loaders for Web\\xa0Data - Comet', 'description': 'The effectiveness of RAG hinges on the method used to retrieve documents. Explore 3 key LangChain document loaders + how they effect output', 'language': 'en-US'}, page_content=\"Graph by author\\nplot_data(newline_chars, 'Number of Newline Characters')\\n\\nGraph by author\\nplot_data(token_count, 'Number of Tokens')\\n\\nGraph by author\\nWhy do we see this difference?\\nThe discrepancy in the number of characters each loader extracted can be attributed to their distinct methodologies and the source code that drives their functionality.\\nHere‚Äôs a breakdown based on the source code and operational differences:\\nWebBaseLoader\\n\\nMethodology: It performs direct HTML fetching using HTTP requests and parses the HTML content with BeautifulSoup.\\nWhy the Difference: This loader extracts all text content from the HTML, including main content, navigation elements, headers, footers, and possibly some script elements. However, it does not execute JavaScript, so any content loaded dynamically (which is common in modern web pages) is not captured. This can lead to a moderate character count\\u200a‚Äî\\u200asubstantial but not exhaustive.\\n\\nSeleniumURLLoader\"),\n",
       " Document(metadata={'source': 'https://www.comet.com/site/blog/langchain-document-loaders-for-web-data/', 'title': 'LangChain Document Loaders for Web\\xa0Data - Comet', 'description': 'The effectiveness of RAG hinges on the method used to retrieve documents. Explore 3 key LangChain document loaders + how they effect output', 'language': 'en-US'}, page_content='SeleniumURLLoader\\n\\nMethodology: Uses Selenium for browser automation, which launches a browser instance (often in headless mode) and interacts with the page like a human user. It can execute JavaScript and capture dynamically loaded content.\\nWhy the Difference: The higher character count is likely due to this loader‚Äôs ability to capture more comprehensive content, including dynamic elements that only load upon user interaction or as a part of JavaScript execution. This method fetches the static HTML content and the additional text that becomes available as the page fully renders in a browser environment. This thorough approach results in capturing a larger volume of text.\\n\\nNewsURLLoader'),\n",
       " Document(metadata={'source': 'https://www.comet.com/site/blog/langchain-document-loaders-for-web-data/', 'title': 'LangChain Document Loaders for Web\\xa0Data - Comet', 'description': 'The effectiveness of RAG hinges on the method used to retrieve documents. Explore 3 key LangChain document loaders + how they effect output', 'language': 'en-US'}, page_content='NewsURLLoader\\n\\nMethodology: Utilizes the newspaper library designed explicitly for scraping and curating news articles. It is optimized for extracting article content while excluding unrelated material.\\nWhy the Difference: The lower character count reflects its focused extraction. The newspaper library targets the core article text and is adept at ignoring extraneous content like ads, sidebars, or site navigation elements. This results in a cleaner and more concise text extraction, focusing primarily on the main news content.\\n\\nSummary\\n\\nWebBaseLoader: Provides a broad capture of HTML content but misses dynamic content, leading to a moderate character count.\\nSeleniumURLLoader: Captures a complete picture of the webpage, including dynamic content, which results in the highest character count.\\nNewsURLLoader: Highly specialized and focused on news content, leading to the lowest character count due to its targeted extraction.'),\n",
       " Document(metadata={'source': 'https://www.comet.com/site/blog/langchain-document-loaders-for-web-data/', 'title': 'LangChain Document Loaders for Web\\xa0Data - Comet', 'description': 'The effectiveness of RAG hinges on the method used to retrieve documents. Explore 3 key LangChain document loaders + how they effect output', 'language': 'en-US'}, page_content='Splitting text for retrieval using RecursiveCharacterTextSplitter\\nRecursiveCharacterTextSplitter is designed to split text into chunks based on a list of separators, which can be tailored for different programming languages or text formats.\\nThe class employs a recursive approach to splitting, ensuring that if one separator doesn‚Äôt result in a split, it falls back to the next one in the list.\\nHere‚Äôs a breakdown of the key components and functionalities of this class:'),\n",
       " Document(metadata={'source': 'https://www.comet.com/site/blog/langchain-document-loaders-for-web-data/', 'title': 'LangChain Document Loaders for Web\\xa0Data - Comet', 'description': 'The effectiveness of RAG hinges on the method used to retrieve documents. Explore 3 key LangChain document loaders + how they effect output', 'language': 'en-US'}, page_content='Separators: The class takes a list of separators (separators) which are used to split the text. The default list includes common separators like new lines and spaces. The separators can be regular expressions if is_separator_regex is set to True.\\nRecursive Splitting: The method _split_text attempts to split the text using the provided separators. If a separator doesn‚Äôt successfully split the text, or if the resulting chunks are too large (exceed the specified chunk size), the method recursively tries with the next separator in the list.\\nLanguage-Specific Separators: The class can adapt its separators based on the programming language of the text, as indicated by the get_separators_for_language method. This method returns a list of separators appropriate for programming languages like Python, Java, C++, etc.'),\n",
       " Document(metadata={'source': 'https://www.comet.com/site/blog/langchain-document-loaders-for-web-data/', 'title': 'LangChain Document Loaders for Web\\xa0Data - Comet', 'description': 'The effectiveness of RAG hinges on the method used to retrieve documents. Explore 3 key LangChain document loaders + how they effect output', 'language': 'en-US'}, page_content='Chunk Size and Merging: The class ensures that the resulting chunks are within a certain size limit (_chunk_size). If smaller chunks are created, they can be merged back together to ensure that each chunk is of a reasonable size.'),\n",
       " Document(metadata={'source': 'https://www.comet.com/site/blog/langchain-document-loaders-for-web-data/', 'title': 'LangChain Document Loaders for Web\\xa0Data - Comet', 'description': 'The effectiveness of RAG hinges on the method used to retrieve documents. Explore 3 key LangChain document loaders + how they effect output', 'language': 'en-US'}, page_content='To use this class effectively:\\n\\nInstantiate the Class: Create an instance of RecursiveCharacterTextSplitter, specify the separators if the default ones are unsuitable for your text.\\nSplit Texts: Use the split_text method to split the texts from each of your loaders.\\nPost-Processing: After splitting, you may need to post-process the chunks, especially if the splitting results in broken sentences or contexts.\\nFurther Analysis: Once the text is split into manageable chunks, you can proceed with your analysis by creating embeddings or pushing them to a vector database.'),\n",
       " Document(metadata={'source': 'https://www.comet.com/site/blog/langchain-document-loaders-for-web-data/', 'title': 'LangChain Document Loaders for Web\\xa0Data - Comet', 'description': 'The effectiveness of RAG hinges on the method used to retrieve documents. Explore 3 key LangChain document loaders + how they effect output', 'language': 'en-US'}, page_content=\"This class is handy when dealing with large text files or texts where a simple split by a single character (like a newline) is insufficient. It allows for a more nuanced and flexible approach to text splitting, catering to the specific structural nuances of different text types.\\ntext_splitter = RecursiveCharacterTextSplitter(\\r\\n    # Set a really small chunk size, just to show.\\r\\n    chunk_size = 250,\\r\\n    chunk_overlap  = 5,\\r\\n    length_function = len\\r\\n)\\r\\n\\r\\ntexts = {\\r\\n    'Web Base Loader': wb_loader_doc[0].page_content,\\r\\n    'Selenium Loader': selenium_loader_doc[0].page_content,\\r\\n    'News URL Loader': newsurl_docs[0].page_content\\r\\n}\\r\\n\\r\\ndef create_chunks(document):\\r\\n    return text_splitter.split_documents(document)\\r\\n\\r\\n# Creating chunks for each document\\r\\nwb_chunks = create_chunks(wb_loader_doc)\\r\\nselenium_chunks = create_chunks(selenium_loader_doc)\\r\\nnewsurl_chunks = create_chunks(newsurl_docs)\\r\\n\\r\\nchunk_counts = {\\r\\n    'WebBase Loader': len(wb_chunks),\"),\n",
       " Document(metadata={'source': 'https://www.comet.com/site/blog/langchain-document-loaders-for-web-data/', 'title': 'LangChain Document Loaders for Web\\xa0Data - Comet', 'description': 'The effectiveness of RAG hinges on the method used to retrieve documents. Explore 3 key LangChain document loaders + how they effect output', 'language': 'en-US'}, page_content=\"selenium_chunks = create_chunks(selenium_loader_doc)\\r\\nnewsurl_chunks = create_chunks(newsurl_docs)\\r\\n\\r\\nchunk_counts = {\\r\\n    'WebBase Loader': len(wb_chunks),\\r\\n    'Selenium Loader': len(selenium_chunks),\\r\\n    'News URL Loader': len(newsurl_chunks)\\r\\n}\\r\\n\\r\\nplot_data(chunk_counts, 'Number of Chunks in Each Document')\"),\n",
       " Document(metadata={'source': 'https://www.comet.com/site/blog/langchain-document-loaders-for-web-data/', 'title': 'LangChain Document Loaders for Web\\xa0Data - Comet', 'description': 'The effectiveness of RAG hinges on the method used to retrieve documents. Explore 3 key LangChain document loaders + how they effect output', 'language': 'en-US'}, page_content='Graph by author\\nRAG Pipeline\\nTo set up your RAG pipeline, you must create vector store retrievers. The following code will do that for you:\\ndef create_index_and_retriever(chunks, embeddings):\\r\\n    \"\"\"\\r\\n    Create an index and retriever for the given chunks using the specified embeddings.\\r\\n\\r\\n    Args:\\r\\n    chunks (list): List of text chunks to be indexed.\\r\\n    embeddings (Embeddings object): Embedding model used for creating the index.\\r\\n\\r\\n    Returns:\\r\\n    retriever (Retriever object): The retriever object for the created index.\\r\\n    \"\"\"\\r\\n    index = FAISS.from_documents(chunks, embeddings)\\r\\n    retriever = index.as_retriever()\\r\\n    return retriever\\r\\n\\r\\n\\r\\n# Embedding and Language Model setup\\r\\nembeddings = OpenAIEmbeddings(show_progress_bar=True)\\r\\nllm = ChatOpenAI(model=\"gpt-4-1106-preview\")\\r\\n\\r\\n# Creating indexes and retrievers\\r\\nwb_retriever = create_index_and_retriever(wb_chunks, embeddings)\\r\\nselenium_retriever = create_index_and_retriever(selenium_chunks, embeddings)'),\n",
       " Document(metadata={'source': 'https://www.comet.com/site/blog/langchain-document-loaders-for-web-data/', 'title': 'LangChain Document Loaders for Web\\xa0Data - Comet', 'description': 'The effectiveness of RAG hinges on the method used to retrieve documents. Explore 3 key LangChain document loaders + how they effect output', 'language': 'en-US'}, page_content='# Creating indexes and retrievers\\r\\nwb_retriever = create_index_and_retriever(wb_chunks, embeddings)\\r\\nselenium_retriever = create_index_and_retriever(selenium_chunks, embeddings)\\r\\nnews_url_retriever = create_index_and_retriever(newsurl_chunks, embeddings)\\nThe following are the questions (queries )and ground truth answers (answers) that you‚Äôll use to assess the performance of each retriever.\\nqueries = [\\r\\n    \"What are educators\\' main concerns regarding using AI chatbots like ChatGPT by students?\",\\r\\n    \"Why do the Stanford researchers believe that concerns about AI chatbots leading to increased student cheating are misdirected?\",\\r\\n    \"What findings have the Stanford researchers gathered about the prevalence of cheating among U.S. high school students in the context of AI chatbots?\",\\r\\n    \"What alternative reasons might explain why students cheat, according to the article?\",'),\n",
       " Document(metadata={'source': 'https://www.comet.com/site/blog/langchain-document-loaders-for-web-data/', 'title': 'LangChain Document Loaders for Web\\xa0Data - Comet', 'description': 'The effectiveness of RAG hinges on the method used to retrieve documents. Explore 3 key LangChain document loaders + how they effect output', 'language': 'en-US'}, page_content='\"What alternative reasons might explain why students cheat, according to the article?\",\\r\\n    \"What recommendations or strategies do the article or researchers suggest for addressing academic dishonesty in schools?\"\\r\\n]\\r\\n\\r\\nanswers = [\\r\\n    \"Educators are concerned about students using AI chatbots like ChatGPT to cheat by passing off AI-generated writing as their own.\",\\r\\n    \"Stanford researchers believe concerns about AI chatbots leading to increased cheating are misdirected because cheating predates these technologies, and when students cheat, it\\'s typically for reasons unrelated to technology access.\",\\r\\n    \"Their research shows that 60% to 70% of students admitted to cheating before the advent of AI chatbots, and this rate has remained constant or even slightly decreased in 2023.\",\\r\\n    \"Alternative reasons for cheating include struggling with material, excessive homework, assignments feeling like busywork, and overwhelming pressure to achieve.\",'),\n",
       " Document(metadata={'source': 'https://www.comet.com/site/blog/langchain-document-loaders-for-web-data/', 'title': 'LangChain Document Loaders for Web\\xa0Data - Comet', 'description': 'The effectiveness of RAG hinges on the method used to retrieve documents. Explore 3 key LangChain document loaders + how they effect output', 'language': 'en-US'}, page_content='\"Alternative reasons for cheating include struggling with material, excessive homework, assignments feeling like busywork, and overwhelming pressure to achieve.\",\\r\\n    \"Recommended strategies include helping students feel more engaged and valued, addressing deeper systemic problems, and promoting a sense of belonging, purpose, and connection in the educational environment.\"\\r\\n]\\nThe QAChainRunner is a pivotal component designed to streamline the querying and retrieving answers using a RetrievalQA chain.\\nThis class, engineered for flexibility and efficiency, is a centralized conduit between the user‚Äôs queries and the complex machinery of language model-based retrieval systems. Upon initialization, it accepts a pre-defined language model (LLM), setting the stage for sophisticated query-processing operations.\\nIn action, the QAChainRunner takes a retriever object and a query as input.'),\n",
       " Document(metadata={'source': 'https://www.comet.com/site/blog/langchain-document-loaders-for-web-data/', 'title': 'LangChain Document Loaders for Web\\xa0Data - Comet', 'description': 'The effectiveness of RAG hinges on the method used to retrieve documents. Explore 3 key LangChain document loaders + how they effect output', 'language': 'en-US'}, page_content='In action, the QAChainRunner takes a retriever object and a query as input.\\nIt then dynamically constructs a RetrievalQA chain, leveraging the power of the provided language model to interpret and process the query. The real strength of this class lies in its ability to handle multiple queries seamlessly, returning a structured and comprehensive set of results. Each result includes the original query, the generated answer, and the source documents that informed the response, offering an insightful peek into the retrieval process.\\nIn essence, QAChainRunner acts as an intelligent intermediary, transforming simple queries into insightful answers, making it an indispensable tool for any application or system focused on advanced information retrieval and question-answering tasks.\\nfrom typing import List, Dict, Any\\r\\nfrom datasets import Dataset\\r\\n\\r\\nclass QAChainRunner:\\r\\n    \"\"\"\\r\\n    Class to handle running queries through a RetrievalQA chain.\\r\\n    \"\"\"\\r\\n    def __init__(self, llm):'),\n",
       " Document(metadata={'source': 'https://www.comet.com/site/blog/langchain-document-loaders-for-web-data/', 'title': 'LangChain Document Loaders for Web\\xa0Data - Comet', 'description': 'The effectiveness of RAG hinges on the method used to retrieve documents. Explore 3 key LangChain document loaders + how they effect output', 'language': 'en-US'}, page_content='from datasets import Dataset\\r\\n\\r\\nclass QAChainRunner:\\r\\n    \"\"\"\\r\\n    Class to handle running queries through a RetrievalQA chain.\\r\\n    \"\"\"\\r\\n    def __init__(self, llm):\\r\\n        self.llm = llm\\r\\n\\r\\n    def run_retrieval_qa(self, retriever, query):\\r\\n        \"\"\"\\r\\n        Run a query through the RetrievalQA chain.\\r\\n\\r\\n        Args:\\r\\n        retriever (Retriever object): The retriever to use.\\r\\n        query (str): The query to process.\\r\\n\\r\\n        Returns:\\r\\n        dict: The response including the query, result, and source documents.\\r\\n        \"\"\"\\r\\n        try:\\r\\n            qa_chain = RetrievalQA.from_chain_type(llm=self.llm, \\r\\n                                                   retriever=retriever, \\r\\n                                                   verbose=True,\\r\\n                                                   return_source_documents=True)\\r\\n            return qa_chain.invoke(query)\\r\\n        except Exception as e:\\r\\n            print(f\"Error in running RetrievalQA: {e}\")'),\n",
       " Document(metadata={'source': 'https://www.comet.com/site/blog/langchain-document-loaders-for-web-data/', 'title': 'LangChain Document Loaders for Web\\xa0Data - Comet', 'description': 'The effectiveness of RAG hinges on the method used to retrieve documents. Explore 3 key LangChain document loaders + how they effect output', 'language': 'en-US'}, page_content='return qa_chain.invoke(query)\\r\\n        except Exception as e:\\r\\n            print(f\"Error in running RetrievalQA: {e}\")\\r\\n            return {\"query\": query, \"result\": None, \"source_documents\": []}\\r\\n\\r\\n    def run_queries(self, retriever, queries: List[str]) -> List[Dict[str, Any]]:\\r\\n        \"\"\"\\r\\n        Run multiple queries through the RetrievalQA chain.\\r\\n\\r\\n        Args:\\r\\n        retriever (Retriever object): The retriever to use.\\r\\n        queries (List[str]): List of queries to process.\\r\\n\\r\\n        Returns:\\r\\n        List[Dict[str, Any]]: List of responses for each query.\\r\\n        \"\"\"\\r\\n        return [self.run_retrieval_qa(retriever, query) for query in queries]\\r\\n\\r\\ndef parse_retrieval_qa_results(results, ground_truths):\\r\\n    \"\"\"\\r\\n    Parse the results from the RetrievalQA pipeline into a structured format.\\r\\n\\r\\n    Args:\\r\\n    results (List[Dict[str, Any]]): Results from the RetrievalQA pipeline.\\r\\n    ground_truths (List[str]): Ground truth answers.\\r\\n\\r\\n    Returns:'),\n",
       " Document(metadata={'source': 'https://www.comet.com/site/blog/langchain-document-loaders-for-web-data/', 'title': 'LangChain Document Loaders for Web\\xa0Data - Comet', 'description': 'The effectiveness of RAG hinges on the method used to retrieve documents. Explore 3 key LangChain document loaders + how they effect output', 'language': 'en-US'}, page_content='Args:\\r\\n    results (List[Dict[str, Any]]): Results from the RetrievalQA pipeline.\\r\\n    ground_truths (List[str]): Ground truth answers.\\r\\n\\r\\n    Returns:\\r\\n    Dict[str, List[Any]]: Parsed results including questions, answers, contexts, and ground truths.\\r\\n    \"\"\"\\r\\n    parsed_results = {\\'question\\': [], \\'answer\\': [], \\'contexts\\': [], \\'ground_truths\\': []}\\r\\n\\r\\n    for i, result in enumerate(results):\\r\\n        query = result.get(\\'query\\')\\r\\n        answer = result.get(\\'result\\')\\r\\n        source_documents = result.get(\\'source_documents\\', [])\\r\\n\\r\\n        # Transform Document objects into a compatible format (e.g., string or dict)\\r\\n        contexts = []\\r\\n        for doc in source_documents:\\r\\n            if hasattr(doc, \\'page_content\\'):\\r\\n                # Assuming doc is a Document object with a \\'page_content\\' attribute\\r\\n                contexts.append(doc.page_content)\\r\\n            elif isinstance(doc, dict):\\r\\n                # If doc is already a dictionary, use as is or convert to string'),\n",
       " Document(metadata={'source': 'https://www.comet.com/site/blog/langchain-document-loaders-for-web-data/', 'title': 'LangChain Document Loaders for Web\\xa0Data - Comet', 'description': 'The effectiveness of RAG hinges on the method used to retrieve documents. Explore 3 key LangChain document loaders + how they effect output', 'language': 'en-US'}, page_content='contexts.append(doc.page_content)\\r\\n            elif isinstance(doc, dict):\\r\\n                # If doc is already a dictionary, use as is or convert to string\\r\\n                contexts.append(str(doc))\\r\\n            else:\\r\\n                # Fallback for other types\\r\\n                contexts.append(str(doc))\\r\\n\\r\\n        parsed_results[\\'question\\'].append(query)\\r\\n        parsed_results[\\'answer\\'].append(answer)\\r\\n        parsed_results[\\'contexts\\'].append(contexts)\\r\\n        parsed_results[\\'ground_truths\\'].append(ground_truths[i] if i < len(ground_truths) else [])\\r\\n\\r\\n    return parsed_results\\r\\n\\r\\n\\r\\ndef create_hf_dataset_from_dict(parsed_results: Dict[str, List[Any]]) -> Dataset:\\r\\n    \"\"\"\\r\\n    Convert parsed results into a Hugging Face Dataset object.\\r\\n\\r\\n    Args:\\r\\n    parsed_results (Dict[str, List[Any]]): Parsed results from the RetrievalQA pipeline.\\r\\n\\r\\n    Returns:\\r\\n    Dataset: A Hugging Face Dataset object.\\r\\n    \"\"\"\\r\\n    try:\\r\\n        return Dataset.from_dict(parsed_results)'),\n",
       " Document(metadata={'source': 'https://www.comet.com/site/blog/langchain-document-loaders-for-web-data/', 'title': 'LangChain Document Loaders for Web\\xa0Data - Comet', 'description': 'The effectiveness of RAG hinges on the method used to retrieve documents. Explore 3 key LangChain document loaders + how they effect output', 'language': 'en-US'}, page_content='Returns:\\r\\n    Dataset: A Hugging Face Dataset object.\\r\\n    \"\"\"\\r\\n    try:\\r\\n        return Dataset.from_dict(parsed_results)\\r\\n    except Exception as e:\\r\\n        print(f\"Error in creating dataset: {e}\")\\r\\n        return None\\nContext Recall and Context Precision in\\xa0ragas\\nContext Recall is a metric for information retrieval and natural language processing, particularly in systems like Retrieval-Augmented Generation (RAG). It measures the effectiveness of a retrieval system in fetching relevant information or documents that contribute meaningfully to generating accurate and contextually appropriate answers to queries.'),\n",
       " Document(metadata={'source': 'https://www.comet.com/site/blog/langchain-document-loaders-for-web-data/', 'title': 'LangChain Document Loaders for Web\\xa0Data - Comet', 'description': 'The effectiveness of RAG hinges on the method used to retrieve documents. Explore 3 key LangChain document loaders + how they effect output', 'language': 'en-US'}, page_content='Context Precision, similar to Context Recall, is a vital metric for evaluating information retrieval systems, particularly in contexts like Retrieval-Augmented Generation (RAG) models. While Context Recall focuses on the proportion of relevant documents retrieved from the total ground truths, Context Precision measures the relevance of the retrieved documents against all the documents retrieved.\\nHow Context Recall\\xa0Works'),\n",
       " Document(metadata={'source': 'https://www.comet.com/site/blog/langchain-document-loaders-for-web-data/', 'title': 'LangChain Document Loaders for Web\\xa0Data - Comet', 'description': 'The effectiveness of RAG hinges on the method used to retrieve documents. Explore 3 key LangChain document loaders + how they effect output', 'language': 'en-US'}, page_content='Information Retrieval: In RAG systems, when a query is posed, the model retrieves a set of documents or contexts that it believes are relevant to the query.\\nAnswer Generation: The model then uses these contexts and language understanding capabilities to generate an answer.\\n\\nWhat Context Recall\\xa0Measures\\n\\nRelevance of Retrieved Contexts: Context Recall assesses how many retrieved documents are relevant or helpful in answering the query.\\nEffectiveness of the Retrieval Component: It evaluates the retrieval component of the model, which is crucial for the overall quality of the answer.\\n\\nHow Context Recall is\\xa0Measured'),\n",
       " Document(metadata={'source': 'https://www.comet.com/site/blog/langchain-document-loaders-for-web-data/', 'title': 'LangChain Document Loaders for Web\\xa0Data - Comet', 'description': 'The effectiveness of RAG hinges on the method used to retrieve documents. Explore 3 key LangChain document loaders + how they effect output', 'language': 'en-US'}, page_content='How Context Recall is\\xa0Measured\\n\\nComparison With Ground Truths: Typically, for each query, there is a set of ground truth documents known to contain relevant information. Context Recall measures how many of these ground truth documents were retrieved by the model.\\nCalculation: It can be calculated as the proportion or count of relevant documents retrieved out of the total ground truth documents. This is often represented as a percentage or a ratio.\\n\\nImportance in AI\\xa0Models\\n\\nImproves Answer Quality: High Context Recall indicates that the model effectively fetches relevant information, which is crucial for generating accurate and comprehensive answers.\\nModel Optimization: By measuring Context Recall, developers can fine-tune the retrieval component of the model for better performance.\\n\\nHow Context Precision Works'),\n",
       " Document(metadata={'source': 'https://www.comet.com/site/blog/langchain-document-loaders-for-web-data/', 'title': 'LangChain Document Loaders for Web\\xa0Data - Comet', 'description': 'The effectiveness of RAG hinges on the method used to retrieve documents. Explore 3 key LangChain document loaders + how they effect output', 'language': 'en-US'}, page_content='How Context Precision Works\\n\\nInformation Retrieval: When a query is posed in a RAG system, the system retrieves a set of documents or contexts.\\nRelevance Assessment: Context Precision assesses how many of these retrieved documents are relevant to the query.\\n\\nWhat Context Precision Measures\\n\\nAccuracy of Retrieved Contexts: It measures the proportion of the retrieved documents relevant to the query.\\nEfficiency of the Retrieval Component: High Context Precision indicates that the model‚Äôs retrieval component is active and accurate, fetching more relevant documents than irrelevant ones.\\n\\nHow Context Precision is\\xa0Measured\\n\\nComparison With Relevant Documents: Context Precision is calculated by dividing the number of relevant documents retrieved by the total number of documents retrieved for each query.\\nCalculation: Often expressed as a percentage, it indicates the retrieval system‚Äôs accuracy.\\n\\nImportance in AI\\xa0Models'),\n",
       " Document(metadata={'source': 'https://www.comet.com/site/blog/langchain-document-loaders-for-web-data/', 'title': 'LangChain Document Loaders for Web\\xa0Data - Comet', 'description': 'The effectiveness of RAG hinges on the method used to retrieve documents. Explore 3 key LangChain document loaders + how they effect output', 'language': 'en-US'}, page_content='Importance in AI\\xa0Models\\n\\nEnhances the Quality of Generated Answers: Context Precision helps generate more accurate and contextually correct answers by ensuring that the retrieved documents are primarily relevant.\\nModel Optimization and Balancing: Alongside Context Recall, Context Precision helps fine-tune RAG models‚Äô retrieval components. A balance between Context Recall and Precision is often sought for optimal performance.\\n\\nfrom ragas import evaluate\\r\\n\\r\\nfrom ragas.metrics import context_recall, context_precision\\r\\n\\r\\nwb_result = evaluate(wb_dataset, metrics=[context_precision, context_recall])\\r\\n\\r\\nselenium_result = evaluate(selenium_dataset, metrics=[context_precision, context_recall])\\r\\n\\r\\nnewsurl_result = evaluate(newsurl_dataset, metrics=[context_precision, context_recall])\\nInterpretation of\\xa0Results\\nI‚Äôm not gonna lie: I am pretty surprised by the results. I was expecting to see the NewsURLLoader win across all metrics.'),\n",
       " Document(metadata={'source': 'https://www.comet.com/site/blog/langchain-document-loaders-for-web-data/', 'title': 'LangChain Document Loaders for Web\\xa0Data - Comet', 'description': 'The effectiveness of RAG hinges on the method used to retrieve documents. Explore 3 key LangChain document loaders + how they effect output', 'language': 'en-US'}, page_content='Graph by author\\nWeb Base Loader‚Äôs Superiority: The Web Base loader has the highest RAGAS score, indicating it‚Äôs the most effective overall in Retrieval Augmented Generation. Its high context precision and recall suggest it‚Äôs adept at retrieving relevant documents without missing many important ones.\\nSelenium Loader‚Äôs Balanced Performance: The Selenium loader shows a slightly lower RAGAS score but maintains a high context recall, equal to the Web Base loader. Its context precision is lower, though, which might suggest it retrieves more documents, but a slightly larger proportion of them are less relevant.\\nNews URL Loader‚Äôs Lower Recall: While matching the Web Base loader in precision, the News URL loader falls behind in context recall and RAGAS score. This could indicate that while it‚Äôs good at finding relevant documents, it misses many relevant ones compared to the other loaders.'),\n",
       " Document(metadata={'source': 'https://www.comet.com/site/blog/langchain-document-loaders-for-web-data/', 'title': 'LangChain Document Loaders for Web\\xa0Data - Comet', 'description': 'The effectiveness of RAG hinges on the method used to retrieve documents. Explore 3 key LangChain document loaders + how they effect output', 'language': 'en-US'}, page_content='The observation that the NewsURLLoader extracts cleaner text yet performs lower in terms of the overall RAGAS score and context recall is quite intriguing and points to a few potential reasons:\\nPrecision vs. Quality of Content: While the NewsURLLoader might be retrieving cleaner, more precise text, the effectiveness of a retrieval system in a Retrieval-Augmented Generation (RAG) setup is not solely determined by text cleanliness. The key is to retrieve content that is not just clean but also highly relevant and comprehensive in answering the query. If the cleaner text is less comprehensive or slightly off-topic, it might contribute less effectively to the answer generation, impacting the RAGAS score.'),\n",
       " Document(metadata={'source': 'https://www.comet.com/site/blog/langchain-document-loaders-for-web-data/', 'title': 'LangChain Document Loaders for Web\\xa0Data - Comet', 'description': 'The effectiveness of RAG hinges on the method used to retrieve documents. Explore 3 key LangChain document loaders + how they effect output', 'language': 'en-US'}, page_content='Nature of Source Documents: The NewsURLLoader might be optimized for extracting text from news websites, which often have cleaner and more structured content. However, if the content from these sources is less diverse and rich in answering a wide array of queries compared to other sources, it might lead to lower recall and RAGAS scores.\\nContext Recall Challenge: The lower context recall score suggests that the NewsURLLoader, despite retrieving high-quality text, might be missing out on a significant number of relevant documents. This could be due to stricter or more conservative retrieval algorithms, which prefer precision over the breadth of retrieval.\\nMatching Query with Context: The effectiveness of a RAG system also depends on how well the retrieved context aligns with the nuances of the query. If the NewsURLLoader‚Äôs algorithm is tuned to favour text cleanliness over nuanced matching, it might retrieve text that, while clean, is not as aligned with the specific needs of the query.'),\n",
       " Document(metadata={'source': 'https://www.comet.com/site/blog/langchain-document-loaders-for-web-data/', 'title': 'LangChain Document Loaders for Web\\xa0Data - Comet', 'description': 'The effectiveness of RAG hinges on the method used to retrieve documents. Explore 3 key LangChain document loaders + how they effect output', 'language': 'en-US'}, page_content='Integration with the RAG System: The overall architecture and integration of the NewsURLLoader with the RAG system could also play a role. Even if the text is cleaner, other aspects, like how the loader interfaces with the language model, the handling of metadata, and the overall synergy with the RAG process, are crucial.'),\n",
       " Document(metadata={'source': 'https://www.comet.com/site/blog/langchain-document-loaders-for-web-data/', 'title': 'LangChain Document Loaders for Web\\xa0Data - Comet', 'description': 'The effectiveness of RAG hinges on the method used to retrieve documents. Explore 3 key LangChain document loaders + how they effect output', 'language': 'en-US'}, page_content='LangChainLanguage ModelsLLMLLMOpsPrompt Engineering\\n\\n\\n\\n \\n\\nHarpreet Sahota\\n\\n\\n\\n\\nRelated Articles\\n\\n\\n\\n\\n\\n \\n\\n\\n\\nBuilding a Low-Cost Local LLM Server to Run 70 Billion Parameter Models\\n\\n\\n\\nFabr√≠cio Ceolin \\n\\n                                August 30, 2024                            \\n\\nA guest post from Fabr√≠cio Ceolin, DevOps Engineer at Comet. Inspired by the growing demand‚Ä¶\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\nHow to Evaluate Your RAG Using the RAGAs Framework\\n\\n\\n\\nAlexandru Razvant and Decoding ML \\n\\n                                July 31, 2024                            \\n\\nWelcome to Lesson 10 of 11 in our free course series, LLM Twin: Building Your‚Ä¶\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\nArchitect Scalable and Cost-Effective LLM & RAG Inference Pipelines\\n\\n\\n\\nPaul Iusztin and Decoding ML \\n\\n                                July 23, 2024                            \\n\\nWelcome to Lesson 9 of 11 in our free course series, LLM Twin: Building Your‚Ä¶\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSubscribe to Comet'),\n",
       " Document(metadata={'source': 'https://www.comet.com/site/blog/langchain-document-loaders-for-web-data/', 'title': 'LangChain Document Loaders for Web\\xa0Data - Comet', 'description': 'The effectiveness of RAG hinges on the method used to retrieve documents. Explore 3 key LangChain document loaders + how they effect output', 'language': 'en-US'}, page_content='July 23, 2024                            \\n\\nWelcome to Lesson 9 of 11 in our free course series, LLM Twin: Building Your‚Ä¶\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSubscribe to Comet\\n\\n\\nSign up to receive the Comet newsletter and never miss out on the latest ML updates, news and events!\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nThank you for subscribing to Comet‚Äôs newsletter!\\n\\n\\nProductsExperiment Management\\nArtifacts\\nModel Registry\\nModel Production Monitoring\\nLLMOps\\nLearnDocumentation\\nResources\\nComet Blog\\nDeep Learning Weekly\\nHeartbeat\\nLLM Course\\nCompanyAbout Us\\nNews and Events\\nCareers\\nContact Us\\nPricingPricing\\nCreate a Free Account\\nContact Sales\\nFollow UsLinkedInTwitterYoutubeFacebookLearnDocumentation\\nResources\\nComet Blog\\nDeep Learning Weekly\\nHeartbeat\\nLLM Course\\nProductsExperiment Management\\nArtifacts\\nModel Registry\\nModel Production Monitoring\\nLLMOps\\nCompanyAbout Us\\nNews and Events\\nCareers\\nContact Us\\nPricingPricing\\nCreate a Free Account\\nContact Sales\\nFollow UsLinkedInTwitterYoutubeFacebook'),\n",
       " Document(metadata={'source': 'https://www.comet.com/site/blog/langchain-document-loaders-for-web-data/', 'title': 'LangChain Document Loaders for Web\\xa0Data - Comet', 'description': 'The effectiveness of RAG hinges on the method used to retrieve documents. Explore 3 key LangChain document loaders + how they effect output', 'language': 'en-US'}, page_content='¬© 2024  Comet ML, Inc. - All Rights Reserved\\nTerms of Service\\nPrivacy Policy\\nCCPA Privacy Notice\\nCookie Settings\\n\\n\\n\\n\\nEnterprise\\nProducts\\n\\nExperiment Management\\nArtifacts\\nModel Registry\\nModel Production Monitoring\\nLLMOps\\n\\n\\nDocs\\nPricing\\nCustomers\\nLearn\\n\\nResources\\nBlog\\n\\n\\nCompany\\n\\nAbout Us\\nNews and Events\\n\\nEvents\\nPress Releases\\n\\n\\nCareers\\nLeadership\\nContact Us\\n\\n\\nLogin\\nGet Demo\\nTry Comet Free\\n\\nBack To Top\\n\\n\\n\\n\\n\\n\\n\\n\\nWe use cookies to collect statistical usage information about our website and its visitors and ensure we give you the best experience on our website. Please refer to our Privacy Policy to learn more.OkPrivacy policy')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS #vector database\n",
    "vectorstoredb = FAISS.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x1df8ee4cd50>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstoredb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Quality of Extracted Text: WebBaseLoader relies on HTML structure and excels with well-structured websites. However, it might struggle with JavaScript-generated dynamic content, which is increasingly common in modern web design. This means the text it retrieves is as good as the HTML it interprets.\\nEfficiency: WebBaseLoader is efficient and fast, handling multiple requests seamlessly. This efficiency translates into quicker embedding of documents into your vector database, which is crucial for large-scale applications.\\nRelevance: The relevance of the extracted text can vary. In cases where websites are loaded with ads or unrelated content alongside the main text, WebBaseLoader might fetch some noise and valuable data. This could impact the precision of your RAG system‚Äôs outputs.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"When extracting text from websites, the WebBaseLoader\" #uses cosine similarity \n",
    "result = vectorstoredb.similarity_search(query)\n",
    "result[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<openai.resources.chat.completions.Completions object at 0x000001DFDB69B510> async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x000001DFDB5C1C90> root_client=<openai.OpenAI object at 0x000001DFDB75D9D0> root_async_client=<openai.AsyncOpenAI object at 0x000001DFDC785C50> model_name='gpt-4o' model_kwargs={} openai_api_key=SecretStr('**********')\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model = 'gpt-4o')\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate #Using Chat Prompt Template\n",
    "#using context so that it answers only based on the most similar document(context)\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Answer the following question based only on the provided context:\n",
    "    <context>\n",
    "        {context} \n",
    "    </context>\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "document_chain = create_stuff_documents_chain(llm,prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableLambda(format_docs)\n",
       "}), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "| ChatPromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\n    Answer the following question based only on the provided context:\\n    <context>\\n        {context}\\n    </context>\\n'), additional_kwargs={})])\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x000001DFDB69B510>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x000001DFDB5C1C90>, root_client=<openai.OpenAI object at 0x000001DFDB75D9D0>, root_async_client=<openai.AsyncOpenAI object at 0x000001DFDC785C50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
       "| StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The SeleniumURLLoader is particularly suited for dealing with what type of websites?\\n\\nBased on the provided context, the SeleniumURLLoader is particularly suited for dealing with dynamic, JavaScript-heavy websites.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.documents import Document #manually providing context\n",
    "document_chain.invoke({\n",
    "    \"input\":\"The SeleniumURLLoader is your toolkit‚Äôs Swiss\",\n",
    "    \"context\": [Document(page_content=\"The SeleniumURLLoader is your toolkit‚Äôs Swiss army knife for dealing with dynamic, JavaScript-heavy websites. It offers a depth of text retrieval unmatched by more straightforward methods but requires more resources and time. A RAG pipeline is the ideal choice when your focus is on comprehensively capturing the essence of modern, interactive web pages.\")]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableBinding(bound=RunnableLambda(lambda x: x['input'])\n",
       "           | VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000001DF8EE4CD50>, search_kwargs={}), kwargs={}, config={'run_name': 'retrieve_documents'}, config_factories=[])\n",
       "})\n",
       "| RunnableAssign(mapper={\n",
       "    answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "              context: RunnableLambda(format_docs)\n",
       "            }), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "            | ChatPromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\n    Answer the following question based only on the provided context:\\n    <context>\\n        {context}\\n    </context>\\n'), additional_kwargs={})])\n",
       "            | ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x000001DFDB69B510>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x000001DFDB5C1C90>, root_client=<openai.OpenAI object at 0x000001DFDB75D9D0>, root_async_client=<openai.AsyncOpenAI object at 0x000001DFDC785C50>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
       "            | StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])\n",
       "  }), kwargs={}, config={'run_name': 'retrieval_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = vectorstoredb.as_retriever() #retriever automatically retrieves the context (no need for cosine similarity)\n",
    "from langchain.chains import create_retrieval_chain\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
    "retrieval_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'**What are the benefits and drawbacks of using SeleniumURLLoader for text retrieval?**\\n\\n**Benefits:**\\n1. **Comprehensive Text Retrieval:** SeleniumURLLoader can capture text that might be missed by other methods, including content that appears due to user interactions or is dynamically loaded by JavaScript.\\n2. **Accuracy and Relevance:** The text retrieved tends to be highly accurate and reflective of the user‚Äôs experience on the website, leading to more relevant and context-rich embeddings in a vector database.\\n\\n**Drawbacks:**\\n1. **Performance Considerations:** SeleniumURLLoader is slower and more resource-intensive than straightforward HTTP request methods, which could lead to longer processing times, especially for large volumes of data.'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = retrieval_chain.invoke({\n",
    "    \"input\":\"SeleniumURLLoader is used when?\"\n",
    "})\n",
    "response['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
